{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# even the variables are not initialized yet\n",
    "x =tf.Variable(initial_value=2, name='EE')\n",
    "\n",
    "f = x**2+ 4*x - 5\n",
    "\n",
    "init = tf.global_variables_initializer()  # prepare an init node\n",
    "\n",
    "# the with clouse just make the session default\n",
    "with tf.Session() as sess_01:\n",
    "    # ses.run(x.initializer)\n",
    "    # x.initializer.run() \n",
    "    \n",
    "    # Instead of manually running the initializer for every single variable, you can use \n",
    "    # the global_variables_initializer() function.\n",
    "    init.run()  # actually initialize all the variables\n",
    "    \n",
    "    # result = ses.run(f)\n",
    "    result = f.eval()\n",
    "    print('hi', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside Jupyter or within a Python shell you may prefer to create an InteractiveSession. The only difference from a regular Session is that when an InteractiveSession is created it automatically sets itself as the default session, so you don’t need a with block (but you do need to close the session manually when you are done with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_02 = tf.Session()\n",
    "sess_02.run(x.initializer)\n",
    "result = sess_02.run(f)\n",
    "print(result)\n",
    "\n",
    "sess_02.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_03 = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()  # prepare an init node\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "\n",
    "sess_03.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess_04 = tf.Session()\n",
    "init = tf.global_variables_initializer()  # prepare an init node\n",
    "\n",
    "init.run(session=sess_04)\n",
    "result = f.eval(session=sess_04)\n",
    "print(result)\n",
    "\n",
    "sess_04.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Any node you create is automatically added to the default graph:\n",
    "x1 = tf.Variable(1)\n",
    "print(x1.graph)\n",
    "x1.graph is tf.get_default_graph()\n",
    "\n",
    "graph_01 = tf.Graph()\n",
    "with graph_01.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "print(x2.graph is graph_01)\n",
    "\n",
    "# graph_01 is only default within the 'with' scope\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data \n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(housing_data_plus_bias)\n",
    "# print(scaler.mean_)\n",
    "scaled_housing_data_plus_bias = scaler.transform(housing_data_plus_bias)\n",
    "trg = housing.target.reshape(-1, 1)\n",
    "trg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression with TensorFlow via Normal Equation\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants and variables take no input (they are called source ops).\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "# the Normal Equation\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    heta_value = theta.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression with TensorFlow via Optimizer\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "batch_size = m\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    # [...] # load the data from disk\n",
    "    X_batch = scaled_housing_data_plus_bias[batch_index:batch_index+batch_size,:]\n",
    "    y_batch = trg[batch_index:batch_index+batch_size,:]\n",
    "    \n",
    "    return X_batch, y_batch\n",
    "\n",
    "# X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "# y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# placeholder nodes don’t actually perform any computation, \n",
    "# they just output the data you tell them to output at runtime.\n",
    "# If you specify None for a dimension, it means “any size.”\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "# the hypothesis\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "# The cost function; Mean Square Average\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    cost_func = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "######################################################################\n",
    "#### gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "#### gradients = tf.gradients(cost_func, [theta])[0]\n",
    "#### training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "#####################################################################\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(cost_func)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', cost_func)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:  # checkpoint every 100 epochs\n",
    "            save_path = saver.save(sess, \"checkpoint_model.ckpt\")\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            #             if epoch % 100 == 0:\n",
    "            #                 print(\"Epoch\", epoch, \"MSE =\", cost_func.eval())\n",
    "            # sess.run(training_op)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "    \n",
    "    file_writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neural Networks using Tensorflow\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/disooqi/data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/disooqi/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/disooqi/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/disooqi/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.9 Val accuracy: 0.8982\n",
      "1 Train accuracy: 0.96 Val accuracy: 0.916\n",
      "2 Train accuracy: 0.88 Val accuracy: 0.9246\n",
      "3 Train accuracy: 0.94 Val accuracy: 0.9322\n",
      "4 Train accuracy: 0.98 Val accuracy: 0.9342\n",
      "5 Train accuracy: 0.98 Val accuracy: 0.939\n",
      "6 Train accuracy: 0.94 Val accuracy: 0.941\n",
      "7 Train accuracy: 0.88 Val accuracy: 0.945\n",
      "8 Train accuracy: 0.94 Val accuracy: 0.9482\n",
      "9 Train accuracy: 0.98 Val accuracy: 0.9504\n",
      "10 Train accuracy: 0.98 Val accuracy: 0.9522\n",
      "11 Train accuracy: 0.98 Val accuracy: 0.9538\n",
      "12 Train accuracy: 0.94 Val accuracy: 0.9568\n",
      "13 Train accuracy: 0.94 Val accuracy: 0.9592\n",
      "14 Train accuracy: 1.0 Val accuracy: 0.96\n",
      "15 Train accuracy: 0.98 Val accuracy: 0.961\n",
      "16 Train accuracy: 0.94 Val accuracy: 0.9634\n",
      "17 Train accuracy: 0.98 Val accuracy: 0.9628\n",
      "18 Train accuracy: 0.94 Val accuracy: 0.9646\n",
      "19 Train accuracy: 0.96 Val accuracy: 0.9652\n",
      "20 Train accuracy: 1.0 Val accuracy: 0.9666\n",
      "21 Train accuracy: 1.0 Val accuracy: 0.9668\n",
      "22 Train accuracy: 0.98 Val accuracy: 0.9684\n",
      "23 Train accuracy: 0.98 Val accuracy: 0.9696\n",
      "24 Train accuracy: 1.0 Val accuracy: 0.9688\n",
      "25 Train accuracy: 0.94 Val accuracy: 0.9698\n",
      "26 Train accuracy: 1.0 Val accuracy: 0.9708\n",
      "27 Train accuracy: 1.0 Val accuracy: 0.971\n",
      "28 Train accuracy: 0.96 Val accuracy: 0.972\n",
      "29 Train accuracy: 1.0 Val accuracy: 0.9718\n",
      "30 Train accuracy: 0.98 Val accuracy: 0.9722\n",
      "31 Train accuracy: 0.98 Val accuracy: 0.9728\n",
      "32 Train accuracy: 0.96 Val accuracy: 0.973\n",
      "33 Train accuracy: 0.96 Val accuracy: 0.9736\n",
      "34 Train accuracy: 0.98 Val accuracy: 0.974\n",
      "35 Train accuracy: 1.0 Val accuracy: 0.9752\n",
      "36 Train accuracy: 1.0 Val accuracy: 0.9756\n",
      "37 Train accuracy: 0.98 Val accuracy: 0.9758\n",
      "38 Train accuracy: 0.96 Val accuracy: 0.976\n",
      "39 Train accuracy: 1.0 Val accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 1200\n",
    "n_hidden2 = 21\n",
    "n_hidden3 = 38\n",
    "n_hidden4 = 11\n",
    "n_hidden5 = 56\n",
    "n_hidden6 = 100\n",
    "n_hidden7 = 7\n",
    "n_hidden8 = 99\n",
    "n_hidden9 = 22\n",
    "n_hidden10 = 29\n",
    "n_hidden11 = 31\n",
    "n_hidden12 = 38\n",
    "n_hidden13 = 20\n",
    "n_hidden14 = 41\n",
    "n_hidden15 = 24\n",
    "n_hidden16 = 28\n",
    "n_hidden17 = 29\n",
    "n_hidden18 = 10\n",
    "n_hidden19 = 76\n",
    "n_hidden20 = 82\n",
    "n_hidden21 = 531\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        \n",
    "        # initializing weights and bias\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        \n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "        \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    # By default, the tf.layers.dense() function (introduced in Chapter 10) \n",
    "    # uses Xavier initialization (with a uniform distribution).\n",
    "    # see implementation of dense in neuron_layer()\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer(mode='FAN_IN') # try mode=\"FAN_AVG\" for Xaviar\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu, kernel_regularizer=he_init)\n",
    "    \n",
    "    # hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "#     hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    \n",
    "#     hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "#     hidden4 = tf.layers.dense(hidden3, n_hidden4, name=\"hidden4\", activation=tf.nn.relu)\n",
    "#     hidden5 = tf.layers.dense(hidden4, n_hidden5, name=\"hidden5\", activation=tf.nn.relu)\n",
    "#     hidden6 = tf.layers.dense(hidden5, n_hidden6, name=\"hidden6\", activation=tf.nn.relu)\n",
    "#     hidden7 = tf.layers.dense(hidden6, n_hidden7, name=\"hidden7\", activation=tf.nn.relu)\n",
    "#     hidden8 = tf.layers.dense(hidden7, n_hidden8, name=\"hidden8\", activation=tf.nn.relu)\n",
    "#     hidden9 = tf.layers.dense(hidden8, n_hidden9, name=\"hidden9\", activation=tf.nn.relu)\n",
    "#     hidden10 = tf.layers.dense(hidden9, n_hidden10, name=\"hidden10\", activation=tf.nn.relu)\n",
    "#     hidden11 = tf.layers.dense(hidden10, n_hidden11, name=\"hidden11\", activation=tf.nn.relu)\n",
    "#     hidden12 = tf.layers.dense(hidden11, n_hidden12, name=\"hidden12\", activation=tf.nn.relu)\n",
    "#     hidden13 = tf.layers.dense(hidden12, n_hidden13, name=\"hidden13\", activation=tf.nn.relu)\n",
    "#     hidden14 = tf.layers.dense(hidden13, n_hidden14, name=\"hidden14\", activation=tf.nn.relu)\n",
    "#     hidden15 = tf.layers.dense(hidden14, n_hidden15, name=\"hidden15\", activation=tf.nn.relu)\n",
    "#     hidden16 = tf.layers.dense(hidden15, n_hidden16, name=\"hidden16\", activation=tf.nn.relu)\n",
    "#     hidden17 = tf.layers.dense(hidden16, n_hidden17, name=\"hidden17\", activation=tf.nn.relu)\n",
    "#     hidden18 = tf.layers.dense(hidden17, n_hidden18, name=\"hidden18\", activation=tf.nn.relu)\n",
    "#     hidden19 = tf.layers.dense(hidden18, n_hidden19, name=\"hidden19\", activation=tf.nn.relu)\n",
    "#     hidden20 = tf.layers.dense(hidden19, n_hidden20, name=\"hidden20\", activation=tf.nn.relu)\n",
    "#     hidden21 = tf.layers.dense(hidden20, n_hidden21, name=\"hidden21\", activation=tf.nn.relu)\n",
    "    \n",
    "    # logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # The sparse_softmax_cross_entropy_with_logits() function is equivalent to \n",
    "    # applying the softmax activation function and then computing the cross entropy\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "########################      Phew! This concludes the construction phase. ###########################\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/home/disooqi/data/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
    "                                           y: mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist_model_final.ckpt\n",
      "0.9722\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./mnist_model_final.ckpt\") \n",
    "    print (accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist_model_final.ckpt\n",
      "[7 2 1 ..., 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./mnist_model_final.ckpt\")\n",
    "    X_new_scaled = mnist.test.images  # some new images (scaled from 0 to 1)\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RNNs in TensorFlow\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of time steps = 2\n",
    "* number of neurons of RNN only layer = 5\n",
    "* number of inputs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "# X2 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons],dtype=tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "# Y2 = tf.tanh(tf.matmul(Y1, Wy) + tf.matmul(X2, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Mini-batch:        instance 0,instance 1,instance 2,instance 3\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1\n",
    "# X2_batch = np.array([[109, 108, 107], [100, 150, 100], [106, -105, 104], [103, 102, 161]]) # t = 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances =  4\n",
      "Number of inputs =  3\n"
     ]
    }
   ],
   "source": [
    "X0_batch.shape\n",
    "print('Number of instances = ', X0_batch.shape[0])\n",
    "print('Number of inputs = ', X0_batch.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "#########################################################\n",
    "#########  First way to represent your inputs  ##########\n",
    "#########################################################\n",
    "# Mini-batch:        instance 0,instance 1,instance 2,instance 3\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1\n",
    "\n",
    "print(X0_batch.shape)\n",
    "\n",
    "\n",
    "#########################################################\n",
    "########  Second way to represent your inputs  ##########\n",
    "#########################################################\n",
    "\n",
    "\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 0\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 1 (padded with a zero vector)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 2\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 3\n",
    "    ])\n",
    "\n",
    "print(X_batch.shape)\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
