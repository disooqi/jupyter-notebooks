{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent\n",
    "\n",
    "* if you have a huge dataset it may slowdown you training step, just use the mini-batch gradient descent\n",
    "\n",
    "Three types of gradient descent:\n",
    "#### Batch Gradient Descent\n",
    "* Vecorizations allows Batch GD to process all M relatively quickly. However, if M is very large BGD still be very slowly\n",
    "* You need to process the entire training set before you the gradient descent can take a one step\n",
    "* BGD is a special case of mini-batch where the <div class=\"text-danger\">size of the mini-batch = the entire training set</div>\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "* The intution her is to let gradient descent start to make some progress even before you finish processing your entire, your training sets.\n",
    "* Mini-batch t: &nbsp;&nbsp;   $X^{\\{t\\}}, Y^{\\{t\\}}$\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "* SGD is a special case of mini-batch where the <span class=\"text-danger\">size of each mini-batch = one training example</span>\n",
    "* on average, SGD will go to a good direction, but sometimes it'll head in the wrong direction as well. As stochastic gradient descent won't ever converge, it'll always just kind of oscillate and wander around the region of the minimum.\n",
    "epoch is a single pass through the entire training set\n",
    "* big downside is you lost the victorization \n",
    "\n",
    "<span style=\"float:right\">&Mfr;&ofr;&hfr;&afr;&mfr;&efr;&dfr; &Efr;&lfr;&dfr;&efr;&sfr;&ofr;&ufr;&kfr;&ifr;</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent Implementation\n",
    "\n",
    "```python\n",
    "# for loop over the number of Mini-batch\n",
    "for e in range(epochs):\n",
    "    for t in range(T):\n",
    "        normal_gradient_descent(X[t], Y[t])\n",
    "    \n",
    "    \n",
    "def normal_gradient_descent(X, Y):\n",
    "    # vectorized implementation\n",
    "    Y_hat = forward_propagation(X)\n",
    "    cost = compute_cost(Y_hat)\n",
    "    dW, db = compute_gradient_using_packprobagation(cost)\n",
    "    \n",
    "    W = W - alpha*dW\n",
    "    b = b - alpha* db\n",
    "    \n",
    "    \n",
    "def compute_cost(Y_hat):\n",
    "    return (1/number_of_examples_in_the_batch) * sum(loss(each_training_example)) + [regularization term]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines for choosing Mini-batch size\n",
    "* If you have a small training set just use the batch gradient descent (< 2000)\n",
    "* because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2.\n",
    "* make sure that the mini-batch fits in your CPU/GPU memory. Which depends in your application and how large is a single example in your training set.\n",
    "* Try a few different powers of two and then see if you can pick one that makes your gradient descent optimization algorithm as efficient as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponentially Weighted (Moving) Averages - EWMA\n",
    "\n",
    "* $V_t = \\beta V_{t-1} + (1-\\beta) \\theta_t $\n",
    "* You can think of $V_t$ as approximately averaging over the last $ \\frac{1}{1-\\beta}$ days\n",
    "* As $\\beta$ gets larger as the curve gets smoother, beacuse we averaging over more days. However, $V_t$ adapts more slowly, when the x changes.\n",
    "* Use bias correction to help up your EWMA in cold starts\n",
    "* bias correction = $ \\frac{1}{1-\\beta^t} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4U1X6wPHvSzcIZWsLpSBN2RcBQdlRZFMWQQSVQQuy\nKYvo4PzGBa06KtZ9X7EqslgdHZdxY9wYl3EXFBUQFLCtIEuhgEDZ2p7fH/cG0pK0aZM0bfp+nidP\nkpOb5OS2fXty7nvfI8YYlFJKha9aoe6AUkqp4NJAr5RSYU4DvVJKhTkN9EopFeY00CulVJjTQK+U\nUmFOA71SSoU5DfRKKRXmNNArpVSYiwx1BwASEhJMSkpKqLuhlFLVysqVK3caYxqXtV2VCPQpKSms\nWLEi1N1QSqlqRUSyfdlOp26UUirMaaBXSqkwp4FeKaXCnAZ6pZQKcxrolVIqzGmgV0qpUMjMhJQU\nqFXLus7MDNpbVYn0SqWUqlEyM2HGDMjPt+5nZ1v3AVJTA/52OqJXSqnKlpZ2PMi75Odb7UFQZqAX\nkRYi8pGIrBWRNSIy126PE5EPRORX+7qR3S4i8oiIbBCRH0Xk1KD0XCmlqqucnPK1+8mXEX0B8Hdj\nTCegDzBHRDoB84Dlxpi2wHL7PsAIoK19mQE8GfBeK6VUdZacXL52P5UZ6I0xW40x39m39wE/A82B\nMcBie7PFwHn27THAEmP5CmgoIkkB77lSSlVX6engcBRvczis9iAo1xy9iKQA3YGvgURjzFb7oW1A\non27OfC729M2220lX2uGiKwQkRW5ubnl7LZSSlVx7lk1CQnWxZVhA5CRAU4niFjXGRlBORAL5ci6\nEZFY4FXgKmPMnyJy7DFjjBERU543NsZkABkAPXr0KNdzlVKqSiuZVbNr1/HHXBk2GRmQlVUp3fFp\nRC8iUVhBPtMY85rdvN01JWNf77DbtwAt3J5+kt2mlFLhLzMTJk8+MavGXX6+tU0Qc+fd+ZJ1I8Cz\nwM/GmAfcHnoTmGzfngy84dZ+iZ190wfY6zbFo5RS4cs1ki8sLHvbwkJr20oI9mJM6bMmInI68D/g\nJ6DIbr4Ba57+ZSAZyAbGG2Py7H8MjwHDgXxgqjGm1GLzPXr0MFqPXilV7aWkWFMz5REfDzt3Vujt\nRGSlMaZHWdv5knXzmTFGjDFdjTHd7MsyY8wuY8wQY0xbY8xQY0yevb0xxswxxrQ2xnQpK8grpVS1\nUVbZgvIGebDm74M8qtczY5VSyheuaZnsbDDGup440cqaSUiA2FjvzxWx/jl4E6QzYl000CullC88\nlS1w2bULDhzw/lxjoFEj748H6YxYFw30SinlC3+DsXuKZUlBOiPWRQO9Ukr5IlDB2O0cpPyoGArq\nxgbtjFgXDfRKKeWL9PRiQbrCjAGnkz116pF6yb3ccNPioJ0R66KBXimlvHHPsklLg8GDfQ/2Xg6+\nGqeT1Z//wIQ73mZNYhsGn9MvcP311pWgv4NSSlVHnrJsvvwSZs2yatN4Ex8Pzz8PS5YUK1xWKLV4\no9tZnDX5EUY9+hm/5+WzcEpPhnduGvSPoitMKaVUZqY1Ys/Jsebi09O9Lw6ybJlVo8bbyVGxscWn\nYtLSyP7zCHPH3cCqhJZ0aFSPO/qmMLxzU+LqRgfzUx2jgV4pVbN5W9bPWypldrb1HB8WD/ms1zBe\nu+dk3l29jchawgPnnsx53ZpTq1YA5vrLQQO9Uqpm8zZyj4jwXrNmxgyIi/OcMpmcTGGR4b731/Pk\nxxtp6IhieOemXH12e5o1rBP4/vtAA71SqmZyTdd4K1tQWGgdePVUDyw/H+rUsebg3f9JOBzsve0O\nrlz0LZ/+kstFvZK55dxOxERGBOcz+EgPxiqlah73A62lKa3oY17eCYuHrHjwWc7dnsSXG3eSPrYz\nd47rEvIgDxrolVI1UWnlDHyVnGwddM3KIv/QEW599B0u/K0eBYWGFy/rQ2rvUjJzKplO3Silah5/\nyxnY67seKSjirR/+4KHlv/B73kEm93Vy7fAO1I2pWqG1avVGKaUqQ3JyxUoKgzVVk57Oz4NHM/OB\nT8jJy6d9Yj1entmXXi3jAtvPANGpG6VU+CtZR37kyGInMwFln/HqcFgnQmVl8e4pQzj/yS84XFDI\nc1N68u5VZ1TZIA++LSW4UER2iMhqt7aXRGSVfckSkVV2e4qIHHR7bEEwO6+UUmXydIbr4sXWmq1u\nB1KZNav018nIwFx8MY8u/5VZz6+kXWI93rridAZ1aIIEogZOEPmylOAAYD+wxBjT2cPj92OtC3ub\niKQAb3varjS6lKBSKmhKW94vPt66zsuzpnN27YL9+0/czunE/PYbd/5nHRmfbmJc9+bcMa4LtaNC\nm1Hj61KCZc7RG2M+tQO4pzcRYDwwuLwdVEqpSlHagVf3E56ysyEqCqKj4ciR4+0OB/nz07nnrbUs\n+iKLS/o6ufXck6v8KN6dv3P0ZwDbjTG/urW1FJHvReQTETnDz9dXSin/lKeO/NGjUK9esSmdrx54\nliGbm7Loiywm93Vyy+jqFeTB/0B/EfCi2/2tQLIxpjvwf8ALIlLf0xNFZIaIrBCRFbm5uX52Qyml\nSnAdgC1vdk1eHmRlYQoL+dcr/2NSTn3qREfwr1l9uXVM50qvUxMIFU6vFJFIYBxwmqvNGHMYOGzf\nXikiG4F2wAkT8MaYDCADrDn6ivZDKaVOULJQWXkkJ7Nhx37S31nLR+tz6dsqngUTT6OBIyrw/awk\n/ozohwLrjDGbXQ0i0lhEIuzbrYC2wCb/uqiUqrFKpkVmZvr2WAXPfM1u2pKZMx7krAc/4Zvf8rjx\nnI4snd6rWgd58GFELyIvAgOBBBHZDPzDGPMsMIHi0zYAA4DbROQoUATMMsbkBbbLSqkawVv5YJeS\nj02aBJ9/Dv37+z5dU7cu1K4NeXk8P+hibu81gcjDUVwxKIXJ/VJIiI0J6EcKlTLTKyuDplcqpU7g\nbX7dtbpTRc9sdb1GevqxBUKe+mQjd/5nHWe2a8xd53chqUFoygmXV8DSK5VSKiR8WNijQkSsFaKA\nb7PyePZ/v/Humm2M6prEQ3/pRmRE+BUM0ECvlKqavNWjcaVLVnREbz9/yZdZ3PzGGho6orhiUBuu\nGto2LIM8aKBXSlVV6eknZs7YVSMBa06+vFPPDgf7b0vnkWU/k/HpJs7qlMgjE7pTJzr0NeODSQO9\nUqpqci2wXXLRblf755/DggVlB3t7lajClBRe+b97uO/3RuSu3cRFvVpw25jORIXpKN5d+H9CpVT1\n5Frqr2SQd6VVLlhgrdtamvh4WLqUbXsOMu7q57lui4MWjerw+uX9uHNc1xoR5EGzbpRSVVF5Tnjy\ntq5rfDyFO3L5YO12bn5jNQcOF5A+tgtjujWrdiUMvNGsG6VU1ec+aneNzt0LjfnCmBODvcPBR/Mf\n59b7PyZrVz4p8Q6WTO9Fh6YeK7KEPQ30SqnQKDlqL2+Ad2eMlRufk8N33Qfw+LirWJ4dRZsmtXgy\n9VTO6pQYthk1vtBAr5QKjUAs0G0rTEnhs/e+YcHHG/ly0y4aFERxzbBWXHZGK6Ija26Ad9E9oJQK\nvJJ1aC6//MS6NP6e+AQUSi2W9B7LGRMfYfLCb9iYu5+0kR35fN5g5gxqo0HepgdjlVKB5cuBVIcD\n6tTxa7omp0Eic8dez/eJbejVMo5L+jo5q1MiMZHhnRPvTg/GKqVCw5cpmfx836dtHA5rfdeXX4Zd\nuzgYGcNbPUYwf9B0JDqKh8Z0DqtMmmDQQK+U8p979kwgZwns4mMFEy7ii6tu5d/fb+G9Nds4cKSQ\nLkkNeCL1VFrEOQL3fmFKA71Syj/+LPJRGqcTsrJ46dsc7rvrv+TuO0y92pGMPqUZY7o1p3fLuGq5\n2lMoaKBXSlVcZqY1rVJYGNjXtWvaPPvZb8x/ey29WsYxf8zJDGzfhNpRNWcOPlD0kLRSyjeeMmlm\nzAhckI+IOLYg94ZHnmHa0XbMf3stIzo35fnpvRneOUmDfAXpiF4pVbbLLy9eQCw727eCYu68lSpw\nWbyYXWMu4KEPf+WFb3JwROUxb0QHLj29ZY0+2SkQytx7IrJQRHaIyGq3tltEZIuIrLIvI90eu15E\nNojIehEZFqyOK6UqSWam56BeniDvdMLSpdao3YO9SS14onlvBt77MS98k0Nq72Q+vmYgs85srUE+\nAHwZ0S8CHgOWlGh/0Bhzn3uDiHTCWkv2ZKAZ8KGItDPGBHgCTylVadLS/MukcdWQd5UXdjtwezgi\nkkfPnMTC3uPIf3c9g9o3Ju2cjrRpUi8AHVcuZQZ6Y8ynIpLi4+uNAf5pjDkM/CYiG4BewJcV7qFS\nKrT8OYM1Ph4efvh4kLevj954E+/Xbs6jZ05iXcPmnNulGTMGtKJz8wYB6LAqyZ/vRFeIyI/21E4j\nu6058LvbNpvtthOIyAwRWSEiK3Jzc/3ohlIqIEoebM3MtNpdS/dVRGzs8SAPfJ+zm2uiO9Nr6gLm\njJnHvpQ2LJzSg0cu6q5BPogqejD2SWA+YOzr+4Fp5XkBY0wGkAFWCYQK9kMpFQglc+Gzs6374HlJ\nP1/Z3wZyduVz0xur+eSXXOrFRDK4YxPOPaUZA9s3IUJz4YOuQoHeGLPddVtEngbetu9uAVq4bXqS\n3aaUqso8lS3Iz4eJE60DqZMnw7Jl3hfkrlULiopOaDbJybz0TQ7z315LLRGuH9GBiX2c1I3RhL/K\nVKGpGxFJcrs7FnBl5LwJTBCRGBFpCbQFvvGvi0qpoCttHj47G558Evbvh9mzISrqxG1EIDq6WFNe\nXCKXXvYQ8177iVNaNOTdvw1g5pmtNciHQJl7XEReBAYCCSKyGfgHMFBEumFN3WQBMwGMMWtE5GVg\nLVAAzNGMG6WqgeRk76N1l127YPFiiImBo0eLP1ZYCA0bWnPyOTn82rkX08+7gW0Ho7h5VAem9EvR\ncgUhpGWKlVKBqVcjQmFBIUu+zOLe99bjiI7k6UtOo3tyozKfqirG1zLFeiaCUjWFK6tGBCIjrWtX\ndk1qKmRkWPPxFXAoIoqHRsxiwD0fcetba+mZEsdbV/bXIF9F6GSZUjVByRG7qz6Ne3ZNaqp1KWt0\nHx8PBw8ee3xbbDwzL7iJHxLbMKBJLDeN6siwk5tqffgqRKdulKoJUlJKn4OvWxcSEqyDssnJMHLk\nsYU+inE4rJE/QFoaKwvqMOv8m8ivW58HUnsw7OSmQfsI6kS6wpRSNZlrIZDsbKu+TFkVJg8csC5Q\nvGBZfLzVlpdn/QNIT8dcfDHLf97BW3d14j8/bSOpYW0yL+lBu0QtW1BVaaBXKtx4m6YpD9c3/V27\nrFH80qWQmsqWPQe5/rlv+fSXXOLrRnNBj5O4dlh7GjqiS389FVIa6JUKN76s2Voe+fmQlsa6IaOZ\n9Ow35B8u4JbRnZjYx6mVJasJ/SkpFS5cWTVl5cNXwMoCB+MXfEktgdfn9GdKf60RX53oiF6pcFCe\nPPgSWTNl+aBNL64cM4+k2BiWTOuli3FXQxrolQoHvk7XOBxW2WCAuXNPzKpxk9UwiVuHzuCj1j05\nuXYBi2b2pXG9mAB1WFUm/e6lVHXkXlI4IcH36Zo6dazr1FSrXIEHfyS3JePsaYyY+igrWnTm+qYH\neTVtlAb5akxH9EpVNyXXby1lVH6CXbuOnyBVopDZ4YhIrh75N97qdCYAA9o15p7zu9K0Qe1A9FqF\nkAZ6paoTb+u3loedReNeyGxLvcZcN/KvfJbSncvXvsfop26nQ9N6enZrmNBAr1R14u/6rS45OeQ9\n9zzvPPkvvkjqyAdteiMY7v3gcS78v1RIqu//e6gqQwO9UtWJP+u32gqlFs+cPZXHNsWzb+ClNM3f\nzcTvl3HpH99yUtrfiy39p8KDBnqlqhNf6saXYnftevz1vOv4n7Mbg1vGcc2w9vYUzcQAdlJVNZp1\no1RV4amMcEKCdXEt2D1ypJUi6Y0IDBlilRsWsa5nzwankzWJrRg9/VG+TjmFO8d1YeGUnnRMqq/z\n8DWALytMLQRGATuMMZ3ttnuB0cARYCMw1RizR0RSgJ+B9fbTvzLGzApCv5UKL97q07hn1GRnWys8\nlbZ+qzGwYQNkZR1r+nHzHl4YMovXv99CI0c0L008VevE1zBllikWkQHAfmCJW6A/G/ivMaZARO4G\nMMZcZwf6t13b+UrLFKsar7ylC5xO79uLQFERxhgyPt3E3e+uo3ZUBCO7JHHd8A6aDx9GAlam2Bjz\nqR3A3dved7v7FXBBeTuolHJT3oOs2dlWQPc0UEtOJv9IAde88iPv/LiVc7okcdf5XahX28Oi3qpG\nCMQc/TTgP273W4rI9yLyiYicEYDXVyr8JSeX/znGWMHencPB92l3MfbxL1j201auG96Bxy7urkG+\nhvMr0ItIGlAAZNpNW4FkY0x34P+AF0TEY0KuiMwQkRUisiI3N9efbihV/aWnl36Q1Rtjjh14PZrS\nkpv+sZSxG+uxO/8Ii6b2YvbA1nqwVVU8vVJEpmAdpB1i7Il+Y8xh4LB9e6WIbATaASdMwBtjMoAM\nsOboK9oPpcKCK3e95KpQtWpBUZH35zmdkJXF3vyjXP7CSj7fsIvpp7fkb2e1IzZGs6eVpUIjehEZ\nDlwLnGuMyXdrbywiEfbtVkBbYFMgOqpU2EtNtbJljLGya5xOK8h7G5E7HJCezsrsPEY+8j+++S2P\n+y48hZtGddIgr4opM9CLyIvAl0B7EdksItOBx4B6wAciskpEFtibDwB+FJFVwCvALGNMXpD6rlTV\n5V5dMiXFul9ae8nnzphxPKvGfS4+IsK6djr55ZFnmHa0Hec/+SUi8K9Z/bjgtJOC+7lUtVRmemVl\n0PRKFVa8LQJSty4cPQpHjhxvczggI6N42QFvqZb2NA3AFxt2MnPpSiIihGn9WzKlfwr19YBrjeNr\neqUGeqUCrbw58e5pkvHx3ssO2/nxb/7wB39/eRUtE+qyaGovmjWs43eXVfUUsDx6pVQ5lTcn3n2w\nVUpt+UMprVjw4S889OGv9GoZx9OX9KBBHR3Fq7JpoFcq0PwsPAYUG+UXSi1e6z6MB0fO5o8Pf2VM\nt2bcfX5XakdFBKCzqibQomZK+cPTwdWK5sS7s/PjN8afxOjLHueas+aQkNiIFy7rzcMTumuQV+Wi\nI3qlKqrkQdfsbJg6FerXt9q8lSjwwc52J/PvZ97kwQ9+ISYqgkfPPZlRXZP05CdVIRrolfJVZqZ1\nQlNODsTFwe7dJ57MdPTo8Xn2Cgb5V085m+tHXMmRd36md8s4HvxLNz3gqvyigV4pX5QcvZdnQe5y\nyBg4kTt6T6B/63huHnUy7ZvWC8r7qJpFA71SvkhLOzEvPpAcDh6/fTH3bq/DqK5JPDC+G9GReghN\nBYb+JinlC3/WavVQYdK16pMR4cueQ7ksLZN7t9fhvG7NeHhCdw3yKqB0RK+ULyqaMulwHF8RKifH\nep30dPaMvZC3LrmOzK+yWbdtH40Kopg7JIW/DmlLRC094KoCSwO9Up64H3hNTrbWal28uHzTN/Hx\n8PDDxcobFBQWseCTjTySvpwjhUV0TKrPPed35dxuzTRlUgWNBnqlSvKUNulaq/Xll48fiHWVEPaW\nRnnwYLG7X2/axe3v/MxPW/ZyTtckLh/Ymk66OLeqBFrrRqmSvNWqiY+3grf7qN41NZORcXxBb3dO\nJ1//dyWPf7yRT3/JpWn92tw4qiOjujYLWvdVzaG1bpQqL9d0jbe5eE8plfn51vy7h8VBCqUW97Qc\nxFMZX5EQG811wzswpV8KdaJ1ikZVLg30SoH30sK+cM3ju/2DOBQRxRVj5vFh296k9k7mplGddA5e\nhYzmcKmay71OzeTJFc+TtzNpXPVt9kfX4bLzb+LDtr25rVk+6WO7aJBXIaUjelVzlCxhsG/f8UVA\nPM2v+8Jezo/UVA4XwX+f+hfzu57Htth47jvpABdcMT5w/VeqgnwK9CKyEGsh8B3GmM52WxzwEpAC\nZAHjjTG7xUoheBgYCeQDU4wx3wW+60qVQzBKGDidkJ7O/gv+woNvr+Wfv8Zz4PTLaNskllcu6Mqp\nyY38fw+lAsDXqZtFwPASbfOA5caYtsBy+z7ACKxFwdsCM4An/e+mUn6qaAkDT6mP0dHw/POQlcUP\nA85h6P2fsPDz3xjWuSnPTenJO389Q4O8qlJ8GtEbYz4VkZQSzWOAgfbtxcDHwHV2+xJj5W1+JSIN\nRSTJGLM1EB1WqkIqWsIgMtIK7AcOWPfdToL6YO12/vri98THRvPa7H501+Cuqih/5ugT3YL3NiDR\nvt0c+N1tu812W7FALyIzsEb8JCcn+9ENpXxQ0RIGR49Cs2awf3+x5iVfZnHLm2vo0rwBz0zuSeN6\nMYHpp1JBEJCsG3v0Xq4zr4wxGcaYHsaYHo0bNw5EN5TyLj3d8zSML9y+Dazfto9LF3/LzW+sYXCH\nJrw4o48GeVXl+RPot4tIEoB9vcNu3wK0cNvuJLtNqdBJTa3wQiDY3zg/WreD0Y9+xte/5XHNsPY8\nNakHjmhNXFNVnz+B/k1gsn17MvCGW/slYukD7NX5eRUSJddzjY/3vq1rtO+hpLC5PZ03Vm1h5tKV\ntE2M5eOrBzJnUButMqmqDZ8CvYi8CHwJtBeRzSIyHbgLOEtEfgWG2vcBlgGbgA3A08DlAe+1Up64\nB/bYWJg40ZqXN8a6/vNP68CqJ/Zi3Cxdal2LgNPJ+keeIfVga+b+cxUdm9Un89LexMfqVI2qXrSo\nmQoPvpYwKG3BbpFjNWsKiwz3vreep/+3idiYSP5+djsu7pVMZISeTK6qDi1qpmoWX/PkjfEe7O25\n+INHCvn7v1ax7KdtjO9xEvNGdCSurpdvAkpVAxroVXgoT568p2DvcHBkfjqPvb+eJV9lsyf/KDee\n05FLz2gV+L4qVcn0e6gKD+U9F8M1J2/Pxec98TQT81vxyH830LtlHK/M6qtBXoUNDfQqPKSnQ1SU\n79s7nZCVBUVFvPv2lwzb3IRVv+/h4QndeGpSD3qkxAWtq0pVNg30qmoomQqZmVm+56emQv36vm1r\nV5wsLDLc8uYaZj3/HU3qxfDa7H6M6da8vD1XqsrTQK9Cz5Ux454KOWNG6cHe/R9DQoJ1Ka0ipds0\nDRkZrDx9BJOe/ZpFX2QxrX9L/j2nP52bNwj4R1OqKtD0ShV63tZodU2vuC/xFxFh1Y4vLU3S2+sA\ne/OPcv3rP7Lsp200dERxzbD2pPZ2BuqTKFWpNL1SVR/eMmZyck7Mj3ctEOJrkHctDAL8tvMAE5/5\nmu1/HuLqs9sx7fSWWsJA1Qj6W65Cz1tlyeTkiteRh2MLg5CaStbOA1yU8RVHCot4ZXY/urVo6F+f\nlapGdI5ehZ7beqvHuEbiFa0jb0/X7Bl7Ibe8uYazH/qUwwWFZF7aW4O8qnF0RK9CLzXVunat55qc\nDCNHWvcrcgzJ/ifxyS+5XPvKD+zcf4TzT23OlYPb0iLOUfbzlQozGuhV6Lgv1p2cfGyaxee6NZ7E\nx5P/4MPcFXsKSxZ+Q9smsTw7uadm1KgaTbNuVPBlZsLcucfTH+PjYfx4WLz4xGDuKiXs6+Ldruwb\np5OC29N5LrkvT326kZ37jzCtf0uuHd6e2lERgfssSlUhvmbd6By9Cq7MTJg6tXjg3rULnnzS84h9\n167Sg3x8fPGc+KVLwRj2/vwrUw+3IX3Zz3RMqs+rs/tx8+hOGuSVQqduVLClpVnrrgZKXh7s3Hns\n7ubd+WS+u46Xvv2dfYeOcvf5XfhLT12DWCl3GuhVcFU0a8Ybu3jZ2j/+5OHlv/DB2u0ADO2YyOWD\n2mhGjVIeVDjQi0h74CW3plbAzUBD4DIg126/wRizrMI9VNVLyQOscXHep2LKc3YrgMPBNzfcxTNL\nVvD+2u3Urx3J7IGtubi3k+YN6wSm/0qFoQoHemPMeqAbgIhEYC0A/jowFXjQGHNfQHqoqo+S2TLZ\n2VZFSVfZgpLKEeSzTu7B/IvSWL4pioaOPK4c3IZLT29FA0c5KlYqVUMFaupmCLDRGJMtJRdXVjWH\np7NYjx71LZPGNbovMcrf2zCBp25cwDN5DqIOC/NGtGVy3xTqROtBVqV8FaismwnAi273rxCRH0Vk\noYg0CtB7qKrEU1lhb/PxrgOozlKKh5VYnPv3hk2Zd8H19J71HE/k1mZU1yQ+unogs85srUFeqXLy\nO49eRKKBP4CTjTHbRSQR2AkYYD6QZIyZ5uF5M4AZAMnJyadle6p1oqomTyc0ORxQp47nUXtEhJUz\nP2lSqdM1RoSvN+TyrxWbefOHLdQSYdypzZnYx8nJzfSEJ6VK8jWPPhCBfgwwxxhztofHUoC3jTGd\nS3sNPWGqmvFWVjg+Hg4e9JwfLwJ168L+/R5f0gB3j/4rCzqdTWxMJGO7N2fOoDY0bVA7oF1XKpxU\n5glTF+E2bSMiSW6PjQVWB+A9VCh4W/XJ2xTNrl2QkWGN4Esyxgry0dEnPHQgqjZpI+eyoNPZpPZO\n5tu0ocw/r7MGeaUCxK8RvYjUBXKAVsaYvXbbUqxsHANkATONMVtLex0d0VdBnqZnXAdKvWXRgDWq\nL+vM1tjYY4uIfJhyGjeOvJJtjkbMHNCKeSM6oAf0lfJNpSw8Yow5AMSXaJvkz2uqKsJTBo1rUOAt\nyEPZNWrsA7N/7DnIEx9v4PmvcuiYVJ/Hz+vMaU49bq9UMOiZscqzQJ/RivUV7+Pew8lcvIL/rtuO\nAab1b8l1I9oTE6mZNEoFiwZ65Zm3VZ8q6EitSOaN+huvdTyThN/3cPnANvylZwutD69UJdBArzxL\nTy+7JrwvJQycTv7ctpNZF93GF4ntuWpoWy4f2IboSC2cqlRl0b+2msxbVg1YC4B4y6BxcTislElv\nnE7WffUjF965jG+SOnD/hadw1dB2GuSVqmT6F1cTZWZCQgJMnGhNzxhjXc+YcWKwX7z4xPVcXQ4c\nsJ47ZIg1urcZYHmn0xk37WGGP/Q//th7kMXTenH+aScF93MppTzSQF/TuNImPWXH5Odbwd99dO8a\n2XsrX5CUe7z0AAAR7ElEQVSfDxs2HCtdsLtOfVKn3M/00fPYWbs+N57TkU+uGUT/NglB+0hKqdLp\nUoI1jbezWktyOKwA71q4G6wpHk+/LyIcOnyU/67bwd3vrmPr3kPcNKoTE3q2ICpCxxJKBYsuJajK\nV3ispPx8mDy5+HOTT1y5aV90HR4bfhl971zO5ZnfUVBoePGyPkzq49Qgr1QVoSP6cFXewmNlcTis\nwG8v6G2ANzoNJH3wdHLrNmJwhyZM6ZdC/zYJRNTSM1uVqgyVcmasqqIyM62gXPIM1vx8K9A7HKWn\nTXqSnw/LlkFGBoU33sg/2o3k+e4jOaVOAc9M688puoSfUlWWfrcOF65pGhGrHLC3MgV5eccPropY\ntWdKS6F0c2jLVl5uP4Dx12byfPeRzBzQitduOleDvFJVnI7ow0HJaZrSpuPi4oqv6bp/f+m1a2wb\n45oz54KbWPfKjyTHObhrXBcm9Dpxzl4pVfVooA8HngqQeRIVBfv2HZ+j97HEwb87DeSGYVcQUyea\np1N7MLRjE60wqVQ1ooE+HPiaSXP0aLletgjh9sHTWdjzPHo6Cnhk7lCSGtSpQAeVUqGkc/ThwEPa\no78ORsaQNmwOC3uex9T4w7yYNlqDvFLVlAb66sZTbnx6uvcyBb6Ijwenk0OR0Sw7bRjXnHctva5Y\nwovdhjOn8SFuvnoskZoTr1S1FYg1Y7OAfUAhUGCM6SEiccBLQArWKlPjjTG7vb2G5tH7yFtufEaG\nddtTSmVZHA6Knsrgg+5DSH/nZ3Ly8qlXO5KzOiYyvmcL+rSKL/s1lFIhUZmLg2cBPYwxO93a7gHy\njDF3icg8oJEx5jpvr6GB3kfeyhc4nZCV5b1EgTcREfzzwRd59EA8W/YcpE2TWNJGduSMtgk6gleq\nGgj1CVNjgIH27cXAx4DXQK985O2ga3a29U+gHEG+UGpx+8BpPLfFQQ9nba4b0YERnZtq2QKlwlAg\n/qoN8L6IrBSRGXZbotuC4NuAxAC8T81QWo340g66ekuVdDisxbjdHIiqzcyxaTzXYwzT+rfkpZl9\nOfeUZhrklQpTgRjRn26M2SIiTYAPRGSd+4PGGCMiJww17X8KMwCSg5A1Ui2VnIN31YgHq4pkerp1\n1quvI3en03oOwIwZHDl0hHfb9+ORfhPYFHcStzXL55LRnQL/OZRSVYrfgd4Ys8W+3iEirwO9gO0i\nkmSM2SoiScAOD8/LADLAmqP3tx/VXmn1aebOtW6npfke5EUgK4sjBUW8t2YbL1+fybd7DIcio2m9\ndxvPtcrnzFl/CexnUEpVSX4FehGpC9Qyxuyzb58N3Aa8CUwG7rKv3/C3o2HNNZL3ljGzaxdMmwZH\njvj+msnJbNlzkOmLvmXdtn20iKvPhAGJnNE2gUHtm1BLK0wqVWP4O6JPBF63T4ePBF4wxrwrIt8C\nL4vIdCAbGO/n+4Q3X0oYlCfIOxysvvEupj3+OQePFPJk6qkMO7mpBnelaii/Ar0xZhNwiof2XcAQ\nf167RvG1hIEv4uP5cP7j/DWnAQ3rCK/M7kf7pvUC9/pKqWpH0yyqgooejLbPaEUEnE52PJfJlY9+\nwKXZsbRqXJd/z+mvQV4ppYG+SqhoCYOHH7ZOlCoqYtO3qxm7rSnvr9nG3CFteWVWP5rUrx3wriql\nqh8N9MFWWl68S2rqiYuB1CrjRxMfD6mp5B8pYPEXWVy44EsOHS3k1dn9+NtZ7agd5dtiIkqp8Kdr\nxgZTabVpUlPL/1y319j75NM816wHi7/IYnf+UXo4G3H3BV1p3Tj2xO2VUmHJ1xIIOqIPNPcR/OTJ\nJwbq/Hwry6bktiVH++6jfDi23J9xOnn5rkUMzm7MQx/+ymnORrwyqy+vzO6nQV4p5ZGO6AOptFG4\nJyLFT4ASgVmz4IknPG5eUFjEzW+u4YWvc+iZ0oh/jD6Zzs0bBKDjSqnqKNRFzWomX5f0cyn5T9YY\nWLAA+vcvNrVTVGR4f+02Hlm+gbVb/2T2wNZcc3Z7zYtXSvlEA30gBSIf3hjrH0ZqKoVFhmU/beWx\n/25g/fZ9tEyoy2MXd2dU12b+v49SqsbQQB9Iyck+L7hdqpwcPv0ll1vfWsPG3AO0aRLLwxO6Mapr\nMyJ0FK+UKicN9IFUWnXJkvPxXhytFUH6uVexaOE3tGpcl8cvPpURnbV8gVKq4jTrxh8ls2bAezD3\nIcgfjIxh5gU3s6j9IKb0S2HZX8/gnK5JGuSVUn7REX1FeasdHx9vVZssDxH2xNRlWuodrEpoSfrY\nzqT2dga+z0qpGkkDfUV5yrDJz4c6dayTonzMvjnUsjWfXH0H9x1KJHtXPk9c1I3hnZOC0GGlVE2l\nUzfl5Zqu8XbQNS/POtEpovQSBNsaN+eB+1+hz5QnmJlTl935R1k0racGeaVUwOmI3pPMTGvEnpMD\ncXFWW16edXvfvtJrwycnWznwkyad8JABvnCewoIzU/ksqSPkCkM7xjGpj5N+reOJ1DVblVJBoIG+\npJJz7+7z7WXNvTscx9dodUu13BtTl4U9xvBu+36sb5xCYv0YruzRgnGnnkRKQt0gfAillDpOSyCU\nVNq0TFlq1YKiIqs+zciRsHgxvzgSmDk2jaxGSfT8Yx3n9Urh/NnjiInU6pJKKf8EvaiZiLQQkY9E\nZK2IrBGRuXb7LSKyRURW2ZeRFX2PSlXW3LsviooAOLRlK4+v3svgyxdy9vQn2Bfj4OXlD/Hy8GZc\nfOWFGuSVUpXKn6mbAuDvxpjvRKQesFJEPrAfe9AYc5//3ask5S1GVorVia2Zdd71bG7YlNO3/cz4\nqWMZ2705ifUnBqCjSilVfhUO9MaYrcBW+/Y+EfkZaB6ojlWq8hYjc2OA3LoNyW7YjLWJrbj7zMk0\nOriPF168nn6/r4ZFVwe2r0opVU4BORgrIilAd+BroD9whYhcAqzAGvXvDsT7+M09myY52Tpwmppa\noWJkRQivdh7M/QMmsa1ewrH2Llt/5dlXb6PJgd3Ha8krpVQI+R3oRSQWeBW4yhjzp4g8CczHGuzO\nB+4Hpnl43gxgBkByRRfHLo/LL7dKALsOPrvOZLU64PPc/A9N25LRexxftejCrroN6fbHOmZ99Qop\nu/8gZfdWWuzdToQpKp6Bo5RSIeRX1o2IRAFvA+8ZYx7w8HgK8LYxpnNprxP0rJvMTO/FxuLjretS\nUicNsLJ5RzL6Xsj7rXvRKKKIQRu+4awfPmL4znWI6/kREVBYaI3kXd8WlFIqSIK+8IiICPAs8LN7\nkBeRJHv+HmAssLqi7xEwaWnei4qVlhsfFUXWSW245bS/8HHrHjSIKOKqQW259IxWxMaMDk5flVIq\nwPyZuukPTAJ+EpFVdtsNwEUi0g1rIJwFzPSrh4FQgTn4DV168ehF83hnXwy1oyJIG9KW1D7JOKL1\nHDOlVPXiT9bNZ4Cn+rnLKt6dIMjMtE5kKiz0+SlvdDqTeWPmEXFIuKRvC2ae2YrE+rWD2EmllAqe\n8B6euvLjfQzymxo1486BU/mgXV96NW/AY6ndaVJPA7xSqnoL70DvLT9exLrYZ7ICvNxlKDedNZtI\nU8g1iQeZcVlvorTImFIqDFTvSFZyhafMzOKPe5ubNwaKijgcEckbHc9k/EV3cu3Iqzjtj3V8dPgL\n5vztAg3ySqmwUX1H9Pa0zKEjBdQ2xsqDnzQJPv8cnnjC2sZDfvyGuJN4ou94/oypy8rmHdjtaEDy\n7q3c+NGzTL3yfCIm3hCCD6OUUsFTfatXpqSwcV8B50x5mEGbVnL+T8sZsvEbRASWLrVy2EvUsFnZ\nrAPTL7iZwloRnLRnO63yNnPRD+/RL/tHagnFpnKUUqqqC3oefcjl5BBdvwnjf/yQZe3785/2/emd\n8xN3vPcYrSdPtrZxnbCUlsaKwrpcMn4+TfJ3s+TFNJL3bi/+eslarkApFZ6q9YjeNS1TILX45ynD\nuG/AJAqlFo+/cTcDdqyHyZNh2TK+O1qHyX+ZT+PYaP7Zaj9N5lxW/CCtw2Et/6dnsiqlqpGg16MP\nufR0K3MGiDRFTFz1H95eNJfmf+YyefytXDH0Cj748DuWxHVmwkV30OjAHjIXzKFJlLGCutNpPd/p\n1CCvlApr1XdEDycWKgP2R9fhiT4XsvjUURyIcQDQN/sHHn/jbuIO/mkF9qysAPVcKaVCx9cRffUO\n9GAdcJ08+YSTovbG1CWrUTOORkTS7Y/1RBr7QGuJ/HmllKquwv9grItryqVEdcoGhw9wyrZfT9y+\nMkoiK6VUFVJ95+jdpaZ6r07pTmvEK6VqoPAI9FD2ak4REXrQVSlVI4VPoE9Pt0bsnjgcsHixBnml\nVI0UPoE+NfV42iRYI3jQ9EmlVI1X/Q/GuktN1YCulFIlBG1ELyLDRWS9iGwQkXnBeh+llFKlC0qg\nF5EI4HFgBNAJa3nBTsF4L6WUUqUL1oi+F7DBGLPJGHME+CcwJkjvpZRSqhTBCvTNgd/d7m+225RS\nSlWykGXdiMgMEVkhIityc3ND1Q2llAp7wcq62QK0cLt/kt12jDEmA8gAEJFcESm+FFTVkgDsDHUn\nSqH9809V7x9U/T5q//xXkT76tJBGUIqaiUgk8AswBCvAfwtcbIxZE/A3qwQissKXwkGhov3zT1Xv\nH1T9Pmr//BfMPgZlRG+MKRCRK4D3gAhgYXUN8kopVd0F7YQpY8wyYFmwXl8ppZRvwqcEQnBlhLoD\nZdD++aeq9w+qfh+1f/4LWh+rxMIjSimlgkdH9EopFeY00LsRkRYi8pGIrBWRNSIy126/RUS2iMgq\n+zIyhH3MEpGf7H6ssNviROQDEfnVvm4Uwv61d9tPq0TkTxG5KpT7UEQWisgOEVnt1uZxn4nlEbtG\n048icmqI+neviKyz+/C6iDS021NE5KDbflwQ7P6V0kevP1MRud7eh+tFZFiI+veSW9+yRGSV3V7p\n+7CU2FI5v4fGGL3YFyAJONW+XQ8rRbQTcAtwdaj7Z/crC0go0XYPMM++PQ+4O9T9tPsSAWzDyvUN\n2T4EBgCnAqvL2mfASOA/gAB9gK9D1L+zgUj79t1u/Utx3y7E+9Djz9T+m/kBiAFaAhuBiMruX4nH\n7wduDtU+LCW2VMrvoY7o3RhjthpjvrNv7wN+pnqUbhgDLLZvLwbOC2Ff3A0BNhpjQnoynDHmUyCv\nRLO3fTYGWGIsXwENRSSpsvtnjHnfGFNg3/0K66TDkPGyD70ZA/zTGHPYGPMbsAGr/lXQlNY/ERFg\nPPBiMPtQmlJiS6X8Hmqg90JEUoDuwNd20xX2V6iFoZwaAQzwvoisFJEZdluiMWarfXsbkBiarp1g\nAsX/uKrKPgTv+6wq1mmahjW6c2kpIt+LyCcickaoOmXz9DOtavvwDGC7MeZXt7aQ7cMSsaVSfg81\n0HsgIrHAq8BVxpg/gSeB1kA3YCvW18BQOd0YcypWCeg5IjLA/UFjfe8LeSqViEQD5wL/spuq0j4s\npqrsM09EJA0oADLtpq1AsjGmO/B/wAsiUj9E3auyP9MSLqL4gCNk+9BDbDkmmL+HGuhLEJEorB9E\npjHmNQBjzHZjTKExpgh4miB/DS2NMWaLfb0DeN3uy3bX1zr7ekeo+udmBPCdMWY7VK19aPO2z8qs\n01RZRGQKMApItYMA9nTILvv2Sqz573ah6F8pP9OqtA8jgXHAS662UO1DT7GFSvo91EDvxp7Lexb4\n2RjzgFu7+9zYWGB1yedWBhGpKyL1XLexDtitBt4EJtubTQbeCEX/Sig2iqoq+9CNt332JnCJnfXQ\nB9jr9tW60ojIcOBa4FxjTL5be2OxFvZBRFoBbYFNld0/+/29/UzfBCaISIyItMTq4zeV3T/bUGCd\nMWazqyEU+9BbbKGyfg8r88hzVb8Ap2N9dfoRWGVfRgJLgZ/s9jeBpBD1rxVWNsMPwBogzW6PB5YD\nvwIfAnEh3o91gV1AA7e2kO1DrH84W4GjWHOd073tM6wsh8exRnk/AT1C1L8NWHO0rt/DBfa259s/\n+1XAd8DoEO5Drz9TIM3eh+uBEaHon92+CJhVYttK34elxJZK+T3UM2OVUirM6dSNUkqFOQ30SikV\n5jTQK6VUmNNAr5RSYU4DvVJKhTkN9EopFeY00CulVJjTQK+UUmHu/wGLcnGIvPf78AAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9da6189e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "m = 10\n",
    "n = 200\n",
    "beta = 0.90\n",
    "theta = list()\n",
    "the_list = list(range(m,n))\n",
    "for i in the_list:\n",
    "    r = random.randint(i-5,i+5)\n",
    "    theta.append(r)\n",
    "V = 0\n",
    "ewma = list()\n",
    "for t in range(len(the_list)):\n",
    "    warm = 1\n",
    "    if  0 <= t <= 5000:\n",
    "        bias_correction = 1-beta**(t+1)\n",
    "    \n",
    "    V = (beta*V + (1-beta)*theta[t])\n",
    "    # print(theta[t], V, V/warming_up)\n",
    "    ewma.append(V/bias_correction)\n",
    "    \n",
    "plt.plot(list(the_list), theta, 'ro')\n",
    "plt.plot(list(the_list), ewma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with Momentum\n",
    "\n",
    "* The momentum algorithm is almost always faster than the standard gradient descent algorithm.\n",
    "* The momentum algorithm is just apply the EWMA to the gradient descent algorithm so it relies on the previous gradient steps.\n",
    "* The most used value of $\\beta$ is $\\beta_1 = 0.9$ which means it depends on the last 10 gradient steps.\n",
    "* In practice, bias correction is ommited since after few iteration algorithm is warmed up.\n",
    "\n",
    "```python\n",
    "# for loop over the number of Mini-batch\n",
    "for e in range(epochs):\n",
    "    for t in range(T):\n",
    "        normal_gradient_descent(X[t], Y[t])\n",
    "\n",
    "VdW = np.zeros_like(dW)\n",
    "Vdb = np.zeros_like(db)\n",
    "    \n",
    "def normal_gradient_descent(X, Y):\n",
    "    # vectorized implementation\n",
    "    Y_hat = forward_propagation(X)\n",
    "    cost = compute_cost(Y_hat)\n",
    "    dW, db = compute_gradient_using_packprobagation(cost)\n",
    "    \n",
    "    VdW = beta * VdW + (1-beta) * dW\n",
    "    Vdb = beta * Vdb + (1-beta) * db\n",
    "    \n",
    "    W = W - alpha * VdW\n",
    "    b = b - alpha * Vdb\n",
    "    \n",
    "    \n",
    "def compute_cost(Y_hat):\n",
    "    return (1/number_of_examples_in_the_batch) * sum(loss(each_training_example)) + [regularization term]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Prop (RMSProp)\n",
    "\n",
    "- use $\\epsilon$ so you don't divide by zero\n",
    "- first proposed in a coursera course by Hinton\n",
    "\n",
    "```python\n",
    "# for loop over the number of Mini-batch\n",
    "SdW = np.zeros_like(dW)\n",
    "Sdb = np.zeros_like(db)\n",
    "\n",
    "for e in range(epochs):\n",
    "    for t in range(T):\n",
    "        normal_gradient_descent(X[t], Y[t])\n",
    "\n",
    "def normal_gradient_descent(X, Y):\n",
    "    # vectorized implementation\n",
    "    Y_hat = forward_propagation(X)\n",
    "    cost = compute_cost(Y_hat)\n",
    "    dW, db = compute_gradient_using_packprobagation(cost)\n",
    "    \n",
    "    SdW = beta_2 * SdW + (1-beta_2) * dW.*dW\n",
    "    Sdb = beta_2 * Sdb + (1-beta_2) * db.*db\n",
    "    \n",
    "    W = W - alpha * dW/np.sqrt(SdW+epsilon)\n",
    "    b = b - alpha * db/np.sqrt(Sdb+epsilon)\n",
    "    \n",
    "    \n",
    "def compute_cost(Y_hat):\n",
    "    return (1/number_of_examples_in_the_batch) * sum(loss(each_training_example)) + [regularization term]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization Algorithm (AKA ADAptive Moment Estimation)\n",
    "\n",
    "- Adam algorithm is one of those rare algorithms that has really stood up, and has been shown to work well across a wide range of deep learning architectures.\n",
    "- In the typical implementation of Adam algorithm you take into consideration the bias correction\n",
    "- The most used value of $\\beta_2$ is $\\beta_2 = 0.999$ which means it depends on the last 1000 gradient steps.\n",
    "- The most used value of $\\epsilon$ is $\\epsilon = 10^{-8} $\n",
    "\n",
    "```python\n",
    "# for loop over the number of Mini-batch\n",
    "VdW = np.zeros_like(dW)\n",
    "Vdb = np.zeros_like(db)\n",
    "\n",
    "SdW = np.zeros_like(dW)\n",
    "Sdb = np.zeros_like(db)\n",
    "\n",
    "iteration = 0\n",
    "for e in range(epochs):\n",
    "    for t in range(T):\n",
    "        iteration =+ 1\n",
    "        normal_gradient_descent(X[t], Y[t], iteration)\n",
    "\n",
    "def normal_gradient_descent(X, Y, t):\n",
    "    # vectorized implementation\n",
    "    Y_hat = forward_propagation(X)\n",
    "    cost = compute_cost(Y_hat)\n",
    "    dW, db = compute_gradient_using_packprobagation(cost)\n",
    "    \n",
    "    VdW = beta_1*VdW + (1-beta_1) * dw\n",
    "    Vdb = beta_1*Vdb + (1-beta_1) * db\n",
    "    \n",
    "    SdW = beta_2*SdW + (1-beta_2) * dW.*dW\n",
    "    Sdb = beta_2*Sdb + (1-beta_2) * db.*db\n",
    "    \n",
    "    VdW_correction = VdW/(1-beta_1**t)\n",
    "    Vdb_correction = Vdb/(1-beta_1**t)\n",
    "    \n",
    "    SdW_correction = SdW/(1-beta_2**t)\n",
    "    Sdb_correction = Sdb/(1-beta_2**t)\n",
    "    \n",
    "    W = W - alpha * VdW_correction/(np.sqrt(SdW_correction)+epsilon)\n",
    "    b = b - alpha * Vdb_correction/(np.sqrt(Sdb_correction)+epsilon)\n",
    "    \n",
    "    \n",
    "def compute_cost(Y_hat):\n",
    "    return (1/number_of_examples_in_the_batch) * sum(loss(each_training_example)) + [regularization term]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay\n",
    "\n",
    "- One of the things that might help speed up your learning algorithm, is to slowly reduce your learning rate over time. We call this learning rate decay.\n",
    "\n",
    "```python\n",
    "# for loop over the number of Mini-batch\n",
    "VdW = np.zeros_like(dW)\n",
    "Vdb = np.zeros_like(db)\n",
    "\n",
    "SdW = np.zeros_like(dW)\n",
    "Sdb = np.zeros_like(db)\n",
    "\n",
    "iteration = 0\n",
    "__alpha__ = 0.2 # initial value\n",
    "decay_rate = 1\n",
    "\n",
    "for e in range(epochs):\n",
    "    for t in range(T):\n",
    "        iteration =+ 1\n",
    "        normal_gradient_descent(X[t], Y[t], iteration, e)\n",
    "\n",
    "def normal_gradient_descent(X, Y, t, e):\n",
    "    # vectorized implementation\n",
    "    Y_hat = forward_propagation(X)\n",
    "    cost = compute_cost(Y_hat)\n",
    "    dW, db = compute_gradient_using_packprobagation(cost)\n",
    "    \n",
    "    VdW = beta_1*VdW + (1-beta_1) * dw\n",
    "    Vdb = beta_1*Vdb + (1-beta_1) * db\n",
    "    \n",
    "    SdW = beta_2*SdW + (1-beta_2) * dW.*dW\n",
    "    Sdb = beta_2*Sdb + (1-beta_2) * db.*db\n",
    "    \n",
    "    VdW_correction = VdW/(1-beta_1**t)\n",
    "    Vdb_correction = Vdb/(1-beta_1**t)\n",
    "    \n",
    "    SdW_correction = SdW/(1-beta_2**t)\n",
    "    Sdb_correction = Sdb/(1-beta_2**t)\n",
    "    \n",
    "    alpha = __alpha__ * 1/ (1+ decay_rate * e) \n",
    "    \n",
    "    W = W - alpha * VdW_correction/(np.sqrt(SdW_correction)+epsilon)\n",
    "    b = b - alpha * Vdb_correction/(np.sqrt(Sdb_correction)+epsilon)\n",
    "    \n",
    "    \n",
    "def compute_cost(Y_hat):\n",
    "    return (1/number_of_examples_in_the_batch) * sum(loss(each_training_example)) + [regularization term]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
