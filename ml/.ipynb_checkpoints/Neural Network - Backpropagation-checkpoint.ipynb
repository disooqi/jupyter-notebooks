{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, arange, cos, exp, pi, zeros, column_stack, ones, newaxis, log, dot, append, zeros_like\n",
    "from numpy.random import permutation, shuffle, random, randint, rand\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize, fmin_bfgs\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400;  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   # 25 hidden units\n",
    "num_labels = 10;          # 10 labels, from 1 to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### =========== Part 1: Loading and Visualizing Data ============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handwritten_digits = loadmat('ex4data1.mat')\n",
    "handwritten_digits.keys()\n",
    "\n",
    "features = handwritten_digits['X']\n",
    "m, n = features.shape\n",
    "\n",
    "org_y = handwritten_digits['y']\n",
    "y = org_y.copy()\n",
    "y[y==10] = 0\n",
    "features.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ================ Part 2: Loading Parameters ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Theta2', '__version__', '__header__', 'Theta1', '__globals__']\n",
      "(25, 401) (10, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Saved Neural Network Parameters ...\n",
    "weight = loadmat('ex3weights.mat')\n",
    "print weight.keys()\n",
    "\n",
    "# Unroll parameters \n",
    "t1 = weight['Theta1'].ravel(order='F')\n",
    "t2 = weight['Theta2'].ravel(order='F')\n",
    "# nn_params = [Theta1(:) ; Theta2(:)];\n",
    "print weight['Theta1'].shape, weight['Theta2'].shape\n",
    "nn_params = np.r_[t1, t2]\n",
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ================ Part 3: Compute Cost (Feedforward) ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights):  0.287629165161   \n",
      "(this value should be about 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_lambda = 0\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, y, _lambda)\n",
    "print 'Cost at parameters (loaded from ex4weights): ', J,'  \\n(this value should be about 0.287629)\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ================== Part 4: Implement Regularization ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights):  0.383769859091   \n",
      "(this value should be about 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_lambda = 1\n",
    "J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, y, _lambda)\n",
    "print 'Cost at parameters (loaded from ex4weights): ', J,'  \\n(this value should be about 0.383770)\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ================ Part 5: Sigmoid Gradient  ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "print 'Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:\\n  '\n",
    "print sigmoidGradient(array([1, -0.5, 0, 0.5, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ================ Part 6: Initializing Pameters ================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============== Part 7: Implement Backpropagation ==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for training a Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1- Randomly initialize weights\n",
    "\n",
    "We usually initialize the weights to small values close to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "def randInitializeWeights(L_in_size, L_out_size):\n",
    "    epsilon_init = np.sqrt(6)/np.sqrt(L_in_size+L_out_size)\n",
    "    epsilon_init = 0.12\n",
    "    return  rand(L_out_size, L_in_size+1) * 2*epsilon_init - epsilon_init\n",
    "\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "print initial_Theta1.shape, initial_Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2- Implement forward propagation to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(x, Theta1, Theta2):\n",
    "    z2 = Theta1.dot(x[:,newaxis])\n",
    "    a2 = np.r_[[[1]], sigmoid(z2)]\n",
    "    \n",
    "    z3 = Theta2.dot(a2)\n",
    "    return sigmoid(z3).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3- Implement code to compute cost function $J(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    return 1/(1+exp(-z))\n",
    "\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, features, y, reg_parameter):\n",
    "    if nn_params.ndim != 1:\n",
    "        return\n",
    "    theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "    Theta1 = nn_params[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "    Theta2 = nn_params[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "    \n",
    "    m, _ = X.shape\n",
    "    a_1 = np.c_[ones((m)), X]\n",
    "    \n",
    "    z_2 = Theta1.dot(a_1.T) # (25, 401) * (401, 5000)\n",
    "    a_tmp = sigmoid(z_2)    # (25, 5000)\n",
    "    \n",
    "    a_2 = np.vstack((ones((m)), a_tmp))\n",
    "    z_3 = Theta2.dot(a_2)\n",
    "    a_3 = sigmoid(z_3)\n",
    "    \n",
    "    #ex_sum = 0\n",
    "    #for i in arange(m):\n",
    "    #    yVec = zeros((num_labels,1))\n",
    "    #    yVec[y[i]] = 1\n",
    "    #    yVec = yVec.ravel()\n",
    "    #    yVec = np.roll(yVec, -1)\n",
    "    #    ex_sum = ex_sum+ np.sum(-yVec*np.log(a_3[:,i]) - (1-yVec)*np.log(1 - a_3[:,i]))\n",
    "    #else:\n",
    "    #    print ex_sum/m\n",
    "    \n",
    "    incidence_y = zeros((y.size, num_labels))\n",
    "    y_1 = y.ravel()\n",
    "    \n",
    "    incidence_y[arange(m), y_1] = 1  # (5000, 10)\n",
    "    incidence_y = np.roll(incidence_y, -1, axis=1)\n",
    "    \n",
    "    reg_term = _lambda *(np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))/(2*m)\n",
    "    \n",
    "    return np.sum(-incidence_y*np.log(a_3.T) - (1-incidence_y)*np.log(1 - a_3.T))/m +reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4- Implement backprop to compute partial derivatives $\\frac{\\partial}{\\partial \\Theta_{jk}^{(l)}} J(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels,features, y, _lambda):\n",
    "    m = y.size\n",
    "    X = np.c_[ones((m)), features]\n",
    "    \n",
    "    incidence_y = zeros((y.size, num_labels))\n",
    "    incidence_y[arange(m), y.ravel()] = 1  # (5000, 10)\n",
    "    incidence_y = np.roll(incidence_y, -1, axis=1)\n",
    "    \n",
    "    if nn_params.ndim != 1:\n",
    "        return\n",
    "    theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "    Theta1 = nn_params[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "    Theta2 = nn_params[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "\n",
    "    Delta2 = zeros_like(Theta2)\n",
    "    Delta1 = zeros_like(Theta1)\n",
    "\n",
    "    for i in arange(m):\n",
    "        \n",
    "        # forward pass\n",
    "        x = X[i,:]\n",
    "    \n",
    "        z2 = Theta1.dot(x[:,newaxis])\n",
    "        a2 = np.r_[[[1]], sigmoid(z2)]\n",
    "    \n",
    "        z3 = Theta2.dot(a2)\n",
    "        hx = sigmoid(z3).ravel()\n",
    "    \n",
    "        # computing the \"error terms\" that measure how much the nodes were responsible for any errors \n",
    "        # in our output\n",
    "        delta3 = hx - incidence_y[i,:]\n",
    "        delta2 = Theta2.T.dot(delta3)[1:] * sigmoidGradient(z2).ravel()\n",
    "    \n",
    "        Delta2 = Delta2 + delta3[:,newaxis].dot(a2.T)\n",
    "        Delta1 = Delta1 + delta2[:,newaxis].dot(x[:,newaxis].T)\n",
    "    \n",
    "    else:\n",
    "        D2 = Delta2/m + _lambda/m * np.c_[zeros((Theta2.shape[0])), Theta2[:,1:]]\n",
    "        D1 = Delta1/m + _lambda/m * np.c_[zeros((Theta1.shape[0])), Theta1[:,1:]]\n",
    "        return np.r_[D1.ravel(order='F'), D2.ravel(order='F')]\n",
    "\n",
    "initial_weights = np.r_[initial_Theta1.ravel(order='F'), initial_Theta2.ravel(order='F')]\n",
    "D = nn_gradient(initial_weights, input_layer_size, hidden_layer_size, num_labels,features, y, _lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# $ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Use gradient checking to compare $\\frac{\\partial}{\\partial \\Theta_{jk}^{(l)}} J(\\Theta)$ computed using packpropagation vs. using numerical estimate of gradient of $J(\\Theta)$ .\n",
    "Then disable gradient checking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_checking(nn_params):\n",
    "    epsilon = 1e-4\n",
    "    grad_vect = zeros_like(nn_params)\n",
    "    for i in arange(nn_params.size):\n",
    "        e_vector = zeros_like(nn_params)\n",
    "        \n",
    "        e_vector[i] = epsilon\n",
    "        \n",
    "        plus = nnCostFunction(nn_params+e_vector,input_layer_size, hidden_layer_size, num_labels, features, y, 0)\n",
    "        minus = nnCostFunction(nn_params-e_vector,input_layer_size, hidden_layer_size, num_labels, features, y, 0)\n",
    "        grad_estimation = (plus - minus)/(2*epsilon)\n",
    "        grad_vect[i] = grad_estimation\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print i\n",
    "    else:\n",
    "        return grad_vect\n",
    "#         print grad_estimation\n",
    "        \n",
    "#         if i > 100:\n",
    "#             print grad_vect\n",
    "#             break\n",
    "    \n",
    "    \n",
    "G = gradient_checking(initial_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.03209914e-02  -2.03209914e-02]\n",
      " [ -2.07964451e-04  -2.07964455e-04]\n",
      " [  3.06427749e-02   3.06427748e-02]\n",
      " ..., \n",
      " [  1.88195829e-01   1.88188456e-01]\n",
      " [  2.79096172e-01   2.79110521e-01]\n",
      " [  2.30014389e-01   2.30004585e-01]]\n",
      "Andrew Ng says that you should see a relative difference that is less than 1e-9, but you got:  0.0001\n"
     ]
    }
   ],
   "source": [
    "print np.c_[D,G][10:]\n",
    "\n",
    "for diff_value in [1e-10,1e-9,1e-8,1e-7,1e-6,1e-5,1e-4,1e-3,1e-2,1e-1,1]:\n",
    "    if np.all(np.abs(D-G)<diff_value):\n",
    "        print 'Andrew Ng says that you should see a relative difference that is less than 1e-9, but you got: ', \n",
    "        print diff_value\n",
    "        break\n",
    "    else:\n",
    "        \"something wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Use gradient descent or advanced optimization menthod with backpropagation to try to minimize $J(\\Theta)$ as a function of parameters $ \\Theta $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "method : str or callable, optional\n",
    "Type of solver. Should be one of\n",
    "    ‘Nelder-Mead’ (see here)\n",
    "    ‘Powell’ (see here)\n",
    "    ‘CG’ (see here)\n",
    "    ‘BFGS’ (see here)\n",
    "    ‘Newton-CG’ (see here)\n",
    "    ‘L-BFGS-B’ (see here)\n",
    "    ‘TNC’ (see here)\n",
    "    ‘COBYLA’ (see here)\n",
    "    ‘SLSQP’ (see here)\n",
    "    ‘dogleg’ (see here)\n",
    "    ‘trust-ncg’ (see here)\n",
    "    custom - a callable object (added in version 0.14.0), see below for description.\n",
    "'''\n",
    "_lambda = 10\n",
    "# initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "# initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);\n",
    "\n",
    "\n",
    "initial_weights = np.r_[initial_Theta1.ravel(order='F'), initial_Theta2.ravel(order='F')]\n",
    "res = minimize(fun=nnCostFunction, x0 =initial_weights, \n",
    "               args=(input_layer_size, hidden_layer_size, num_labels,features, y, _lambda), method='CG', \n",
    "               jac=nn_gradient, options={'maxiter':30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.939999999999998"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1_size = (input_layer_size+1) * hidden_layer_size\n",
    "opt_Theta1 = res.x[:theta1_size].reshape((hidden_layer_size,input_layer_size+1), order='F') # (25, 401)\n",
    "opt_Theta2 = res.x[theta1_size:].reshape((num_labels, hidden_layer_size+1), order='F') # (10, 26)\n",
    "\n",
    "def predict_from_three_layer_NN(Theta1, Theta2, X):\n",
    "    m, _ = X.shape\n",
    "    A_1 = np.c_[ones((m)), X] # (5000, 400)\n",
    "    \n",
    "    Z_2 = Theta1.dot(A_1.T) # (25, 401) * (401, 5000)\n",
    "    A_tmp = sigmoid(Z_2).T # (5000, 25)    \n",
    "    A_2 = np.c_[(ones((m)), A_tmp)] # (5000, 26) \n",
    "    \n",
    "    Z_3 = Theta2.dot(A_2.T) # (10, 26) * (26, 5000) \n",
    "    A_3 = sigmoid(Z_3).T # (5000, 10)\n",
    "    \n",
    "    return A_3\n",
    "\n",
    "pred = predict_from_three_layer_NN(opt_Theta1, opt_Theta2, features)\n",
    "np.mean(pred.argmax(axis=1)+1 == org_y.ravel())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_lambda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
