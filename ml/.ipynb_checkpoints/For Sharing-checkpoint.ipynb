{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import codecs\n",
    "from collections import Counter\n",
    "import random \n",
    "import numpy as np\n",
    "from numpy.random import permutation, shuffle, rand\n",
    "from numpy.linalg import svd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-24fdff144d0c>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-24fdff144d0c>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    return ' '.join(tmp_sentence)sentences = list()\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def from_buck_to_utf8(text):\n",
    "    b2a = {'A': u'\\u0627',  '<': u'\\u0625',  '|': u'\\u0622',  '>': u'\\u0623',  \"'\": u'\\u0621',  'b': u'\\u0628',  \n",
    "           't': u'\\u062a',  'v': u'\\u062b',  'j': u'\\u062c',  'H': u'\\u062d',  'x': u'\\u062e',  'd': u'\\u062f',  \n",
    "           '*': u'\\u0630',  'r': u'\\u0631',  'z': u'\\u0632',  's': u'\\u0633',  '$': u'\\u0634',  'S': u'\\u0635',  \n",
    "           'D': u'\\u0636',  'T': u'\\u0637',  'Z': u'\\u0638',  'E': u'\\u0639',  'g': u'\\u063a',  'f': u'\\u0641',  \n",
    "           'q': u'\\u0642',  'k': u'\\u0643',  'l': u'\\u0644',  'm': u'\\u0645',  'n': u'\\u0646',  'h': u'\\u0647',  \n",
    "           'w': u'\\u0648',  'y': u'\\u064a',  'Y': u'\\u0649',  'p': u'\\u0629',  '&': u'\\u0624',  '}': u'\\u0626',  \n",
    "           'a': u'\\u064e',  'F': u'\\u064b',  'u': u'\\u064f',  'N': u'\\u064c',  'i': u'\\u0650',  'K': u'\\u064d',  \n",
    "           'o': u'\\u0652',  '~': u'\\u0651'}\n",
    "    text = text.strip().split()\n",
    "    tmp_sentence = list()\n",
    "    for word in text:\n",
    "        tmp_word = list()\n",
    "        for c in word:\n",
    "            tmp_word.append(b2a.get(c,c))\n",
    "        else:\n",
    "            tmp_sentence.append(''.join(tmp_word))\n",
    "    else:\n",
    "        return ' '.join(tmp_sentence)sentences = list()\n",
    "removed_sentences = list()\n",
    "removed_sentences_labels = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "dataset = dict()\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open('/home/disooqi/qcri/dialects/task/DSL-training/task2-train.txt') as training:\n",
    "    LAV = list()\n",
    "    MSA = list()\n",
    "    EGY = list()\n",
    "    GLF = list()\n",
    "    NOR = list()\n",
    "    for i, line in enumerate(training):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        utf8_sentence = sentence_label[0]\n",
    "        \n",
    "        # labels.append(sentence_label[2])\n",
    "        \n",
    "        if len(utf8_sentence.strip().split()) <= 0:\n",
    "            removed_sentences.append(utf8_sentence)\n",
    "            removed_sentences_labels.append(sentence_label[2])\n",
    "#             print i, sentence_label[0]\n",
    "            continue\n",
    "        \n",
    "        sentences.append(utf8_sentence)\n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(utf8_sentence)\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(utf8_sentence)\n",
    "        else:\n",
    "            print(utf8_sentence)\n",
    "    else:\n",
    "#         print 'sentence count:', len(sentences)\n",
    "#         print set(labels)\n",
    "        dataset['LAV'] = LAV\n",
    "        dataset['MSA'] = MSA\n",
    "        dataset['EGY'] = EGY\n",
    "        dataset['GLF'] = GLF\n",
    "        dataset['NOR'] = NOR\n",
    "        LAV = list()\n",
    "        MSA = list()\n",
    "        EGY = list()\n",
    "        GLF = list()\n",
    "        NOR = list()\n",
    "\n",
    "target_names = dataset.keys()\n",
    "print target_names\n",
    "\n",
    "def divide_dataset(dataset ,CV=True, train_perc=80 , CV_perc=0, test_perc=20):\n",
    "    if train_perc + CV_perc + test_perc != 100:\n",
    "        print 'the sum of percs is not 100'\n",
    "        return\n",
    "    samples_train = dict()\n",
    "    samples_cv = dict()\n",
    "    samples_test = dict()\n",
    "    \n",
    "    for dialect, sentences in dataset.items():\n",
    "        samples = permutation(sentences)\n",
    "        train_len = int(np.ceil(len(samples)*(train_perc/100.0)))\n",
    "        samples_train[dialect] = sentences[:train_len]\n",
    "        cv_len = 0\n",
    "        if CV:\n",
    "            cvp = CV_perc/(100.0-60)\n",
    "            cv_len = int(np.ceil((len(samples)-train_len) * cvp))\n",
    "            samples_cv[dialect] = sentences[train_len:train_len+cv_len]\n",
    "            samples_test[dialect] = sentences[train_len+cv_len:]\n",
    "        else:\n",
    "            samples_cv[dialect] = list()\n",
    "            samples_test[dialect] = sentences[train_len:]\n",
    "    else:\n",
    "        return samples_train, samples_cv, samples_test\n",
    "            \n",
    "\n",
    "train_set, cv_set, test_set = divide_dataset(dataset, CV=False, train_perc=80 ,CV_perc=0, test_perc=20)\n",
    "\n",
    "target_names = ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']\n",
    "\n",
    "t,c,ts = 0,0,0\n",
    "for dial in ['LAV', 'MSA', 'EGY', 'GLF', 'NOR']:\n",
    "    t += len(train_set[dial])\n",
    "    c += len(cv_set[dial])\n",
    "    ts+= len(test_set[dial])\n",
    "    print dial, 'training dataset: ', len(train_set[dial]), ', cross-validation set: ', \\\n",
    "    len(cv_set[dial]),', test:', len(test_set[dial])\n",
    "    \n",
    "else:\n",
    "    print 70*'-'\n",
    "    print 'Total  ...  Training: ', t, ', cross-validation data', c, ', test: ', ts\n",
    "\n",
    "dataset_train = train_set['LAV']+train_set['MSA']+train_set['EGY']+train_set['GLF']+train_set['NOR']\n",
    "dataset_cv = cv_set['LAV']+cv_set['MSA']+cv_set['EGY']+cv_set['GLF']+cv_set['NOR']\n",
    "dataset_test = test_set['LAV']+test_set['MSA']+test_set['EGY']+test_set['GLF']+test_set['NOR']\n",
    "\n",
    "\n",
    "label_train = ['LAV' for x in train_set['LAV']] + ['MSA' for x in train_set['MSA']] +\\\n",
    "['EGY' for x in train_set['EGY']] + ['GLF' for x in train_set['GLF']]+['NOR' for x in train_set['NOR']]\n",
    "\n",
    "label_cv = ['LAV' for x in cv_set['LAV']] + ['MSA' for x in cv_set['MSA']] +\\\n",
    "['EGY' for x in cv_set['EGY']] + ['GLF' for x in cv_set['GLF']]+['NOR' for x in cv_set['NOR']]\n",
    "\n",
    "label_test = ['LAV' for x in test_set['LAV']] + ['MSA' for x in test_set['MSA']] +\\\n",
    "['EGY' for x in test_set['EGY']] + ['GLF' for x in test_set['GLF']]+['NOR' for x in test_set['NOR']]\n",
    "\n",
    "train_set, cv_set, test_set = 0,0,0\n",
    "#print len(label_train),len(label_cv),len(label_test)\n",
    "\n",
    "train_zipped = zip(dataset_train, label_train)\n",
    "random.shuffle(train_zipped)\n",
    "dataset_train, label_train = zip(*train_zipped)\n",
    "\n",
    "if dataset_cv:\n",
    "    cv_zipped = zip(dataset_cv, label_cv)\n",
    "    random.shuffle(cv_zipped)\n",
    "    dataset_cv, label_cv = zip(*cv_zipped)\n",
    "\n",
    "if dataset_test:\n",
    "    dataset_test.extend(removed_sentences)\n",
    "    label_test.extend(removed_sentences_labels)\n",
    "    test_zipped = zip(dataset_test, label_test)\n",
    "    random.shuffle(test_zipped)\n",
    "    dataset_test, label_test = zip(*test_zipped)\n",
    "\n",
    "print len(dataset_test), len(removed_sentences), len(label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'char',lowercase=False, max_df=0.95,ngram_range=(2,5), smooth_idf=False,\n",
    "                             sublinear_tf=True)\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "X_train = vectorizer.transform(dataset_train)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train.toarray(), label_train)\n",
    "# svd = TruncatedSVD(n_components=640, random_state=42)\n",
    "# u,s,v = svd.fit(X_train) \n",
    "# train_pred = lda.predict(X_train)\n",
    "# test_pred = lda.predict(X_test)\n",
    "\n",
    "# print 'Training Acc: ',np.around(np.mean(pred_train == label_train)*100,2), '%'\n",
    "# print 'Testing Acc: ',np.around(np.mean(pred_test == label_test)*100,2), '%'\n",
    "print type(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
