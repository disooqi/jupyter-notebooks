{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://ttg.uni-saarland.de/vardial2016/dsl2016.html\n",
    "## https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence count: 7619\n",
      "set(['LAV', 'MSA', 'EGY', 'GLF', 'NOR'])\n"
     ]
    }
   ],
   "source": [
    "sentences = list()\n",
    "labels = list()\n",
    "\n",
    "labels_dist = set()\n",
    "\n",
    "LAV = list()\n",
    "MSA = list()\n",
    "EGY = list()\n",
    "GLF = list()\n",
    "NOR = list()\n",
    "\n",
    "#We will release training and testing data for the following Arabic dialects: \n",
    "# Egyptian, Gulf, Levantine, and North-African, and Modern Standard Arabic (MSA)\n",
    "\n",
    "with codecs.open('task2-train.txt') as training:\n",
    "    for i, line in enumerate(training):\n",
    "        sentence_label = line.strip().split('\\t')\n",
    "        sentences.append(sentence_label[0])\n",
    "        labels.append(sentence_label[2])\n",
    "        \n",
    "        if sentence_label[2] == 'LAV':\n",
    "            LAV.append(sentence_label[0])\n",
    "        elif sentence_label[2] == 'MSA':\n",
    "            MSA.append(sentence_label[0])\n",
    "        elif sentence_label[2] == 'EGY':\n",
    "            EGY.append(sentence_label[0])\n",
    "        elif sentence_label[2] == 'GLF':\n",
    "            GLF.append(sentence_label[0])\n",
    "        elif sentence_label[2] == 'NOR':\n",
    "            NOR.append(sentence_label[0])\n",
    "        else:\n",
    "            print(sentence_label[0])\n",
    "    else:\n",
    "        print 'sentence count:', len(sentences)\n",
    "        print set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$>ny tlthA stHZY >h yA xyr Aldyn wlkn >h',\n",
       " \"$A' Allh wynzl AlklyAt >h lA nErD Elyhm wAlfkrp wnntZr wTAEtnA wnHAwl <nh lm ytm ElY mn dlwqt\",\n",
       " '$A*A b|t$h >nt zrt lyh l>n h*A Ebd Allh bn mHmwd SAHb AlmAl wSAHb AltjArp wlA tfkr >nt >nh bryp wjwyp lmnkwby wwSf fyh wbyqwl Hb AlnAs tEAlwA h*A wld Alb$r Ally mvlA h*h Alsnp trknAh wnEyd mnh >bw Ely jdA mnkm',\n",
       " '$Ahd AlgrAfyk tfAqmh',\n",
       " \"$Ahd tglb wAjb |xr mr Ebr EddA mn brnAmjh <HnA wyAhm lA ymnE xrwj bAlb$r ftzydh whlA Em lxrwqAt AlHq Al$yx xAld Hqq mEy lA yglq fyjb hdf AlnAtw bAlAxtyAr mn Altwqf lA tqAs bqyt Endk nsmH lkl $y' HtY tqrr trHb\",\n",
       " \"$AhdnA Edyd Al>HdAv ElY xlfyp mnE Al$rq En AntmA' dyny b$ yEqd m&tmr >h vAlv yzyl kl bAb kAnt sbqth bED AlmHAwlAt fy nSb AlxyAm AldEwyp bED AlmsAjd >yh AHtSlt ndwAt SHfyp m$ddyn rbmA fy twns wAllh fyh bED AlDbAbyp hl >nh >Hmlhm bAlHq mhddyn fy xTAb dyny mt$dd w>y dwr llxTAb Aldyny b$y' sAm fy nb* AlEnf wAltTrf wAltSdy ll<rhAb fy h*A Altqryr y$wfwA mE bEDnA fy mlxSh kl Ally b$ nHky fy b$ ykwn lnA Ewdp mE mjmwEp mn Almdxnyn fy h*A Al$>n b$ nHky >kvr tEmqA\",\n",
       " '$AHnp',\n",
       " '$AmbAs >nA >kyd Alxmsp bryng mn$q Dmn mn Al>sf kAnt lltbrk <ynAs >nA fy kl AlHAlAt qEd jmEp qAl lnA HAjp qlp lmnTqp',\n",
       " \"$AnzAy sbq wH*rt <nk tbtEd En AxtbA'\",\n",
       " '$Ark br>yk fy AlmxAzn AHtkAr Hml yEny']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOR', 'EGY', 'GLF', 'GLF', 'NOR', 'NOR', 'NOR', 'NOR', 'GLF', 'GLF']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_buck_to_utf8(text):\n",
    "    b2a = {'A': u'\\u0627',  '<': u'\\u0625',  '|': u'\\u0622',  '>': u'\\u0623',  \"'\": u'\\u0621',  'b': u'\\u0628',  \n",
    "           't': u'\\u062a',  'v': u'\\u062b',  'j': u'\\u062c',  'H': u'\\u062d',  'x': u'\\u062e',  'd': u'\\u062f',  \n",
    "           '*': u'\\u0630',  'r': u'\\u0631',  'z': u'\\u0632',  's': u'\\u0633',  '$': u'\\u0634',  'S': u'\\u0635',  \n",
    "           'D': u'\\u0636',  'T': u'\\u0637',  'Z': u'\\u0638',  'E': u'\\u0639',  'g': u'\\u063a',  'f': u'\\u0641',  \n",
    "           'q': u'\\u0642',  'k': u'\\u0643',  'l': u'\\u0644',  'm': u'\\u0645',  'n': u'\\u0646',  'h': u'\\u0647',  \n",
    "           'w': u'\\u0648',  'y': u'\\u064a',  'Y': u'\\u0649',  'p': u'\\u0629',  '&': u'\\u0624',  '}': u'\\u0626',  \n",
    "           'a': u'\\u064e',  'F': u'\\u064b',  'u': u'\\u064f',  'N': u'\\u064c',  'i': u'\\u0650',  'K': u'\\u064d',  \n",
    "           'o': u'\\u0652',  '~': u'\\u0651'}\n",
    "    text = text.strip().split()\n",
    "    tmp_sentence = list()\n",
    "    for word in text:\n",
    "        tmp_word = list()\n",
    "        for c in word:\n",
    "            tmp_word.append(b2a.get(c,c))\n",
    "        else:\n",
    "            tmp_sentence.append(''.join(tmp_word))\n",
    "    else:\n",
    "        return ' '.join(tmp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "utf8 = [from_buck_to_utf8(text) for text in sentences[:10]]\n",
       "\n",
       "for utf8_sentence in utf8:\n",
       "    print <h4 align=\"right\">utf8_sentence<h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "utf8 = [from_buck_to_utf8(text) for text in sentences[:10]]\n",
    "\n",
    "for utf8_sentence in utf8:\n",
    "    print <h4 align=\"right\">utf8_sentence<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'LAV': 1758, 'GLF': 1672, 'NOR': 1612, 'EGY': 1578, 'MSA': 999})\n"
     ]
    }
   ],
   "source": [
    "print Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شارون أعلن حرب مفتوحة شارون يريد إبادة الشعب الفلسطيني واغتيال القادة الفلسطينيين والكوادر الفلسطينيين وجه من خلال ما هذه الجريمة اليوم رسالة لكل الشعب الفلسطيني والأمة العربية بأنه أعلن الحرب وحرب مفتوحة\n",
      "شباب حرموا التعليم وحرموا الثقافة وسليمه كافة الحقوق لم يعد أمامهم إلا أن ينتهي مهنة باستعراض القوة أو بسط السيطرة والنفوذ فواجب وتجاوبا مع سلطة جديدة تحاول أن تجد أدوات التي تسيطر بها فهي لا تستطيع أن تسيطر بالسياسة ولا بالثقافة ولا بأي منطق وإنما تسيطر بالبطل\n",
      "شهد السكندريون نظاما حديديا يتهاوى ورائي سنية فر رئيسا طالما قبضة بقواه الأمنية على شعبه ولم تكن القبضة الأمنية على السكندريين أقل رغم فى ودماء خالد سعيد\n",
      "شكرا الزميل محمد العلمي وقد انضم إلينا مشكورا من أمام المحكمة العليا في واشنطن أه أه أرحب مجددا بضيفي الدكتور صفا رفقة وتوم حرب من أورلاندو فلوريدا الدكتور رفقة بداية أنت كطبيب هل هذا القرار الذي أصدرته المحكمة العليا يخدم مجموع المجتمع الأمريكي أن أنه يقوض مستقبل الأجيال القادمة كما يقول الجمهوريين\n",
      "شكرا جزيلا النفوذ من وقتكم أكثر من هذا ومشاهدينا نتوقف إذا مع عينة أخرى أيضا أكثر تنوعا من آراء المدرسين عن حالة التعليم في مصر\n"
     ]
    }
   ],
   "source": [
    "for sentence in MSA[:5]:\n",
    "    print from_buck_to_utf8(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شارك\n",
      "شايف ليش ما أنت تصنع للمواد وسيرت بكل مهام للبلد ومعامل للدول المجاورة واللي بعد المجاورة كما وأحسن ما تدفع حق بالعملة الصعبة أضعف تكلفتها\n",
      "شعبي كاسح من كثير عطشا\n",
      "شعور ناحية الحق عليك أن تراها العماري\n",
      "شفنا بيعملوا بالشكل اللي بيحبوا\n"
     ]
    }
   ],
   "source": [
    "for sentence in LAV[:5]:\n",
    "    print from_buck_to_utf8(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شاء الله وينزل الكليات أه لا نعرض عليهم والفكرة وننتظر وطاعتنا ونحاول إنه لم يتم على من دلوقت\n",
      "شارك طبعا من استهلاك نوع مختلف عن الثاني فى بالدين أحدث تلفيات في أكثر من حاجة ما ننظر مناسبة كمراكز المجال الثاني كنا نحتاج لشخصية صلاح بنشوف حاجات تشابك لكل وحدتنا وصل اليوم عن طريق نتفاوض ما بيصيرش إنا في سيتسم صايل فبص ناصيته ويجب من من هنا لهنا محتجين على غرار واتصلنا به ووجدنا صلاح من الواضح دية اللي إحنا كنا محتاجينها تصلح ضروري جدا في الشركة وذكر طبعا كان أقرب وأسهل طريق نحن نقدر نوصل من خلال\n",
      "شارك تفسر أي شيء بضاعته واحد أي شيء مما يفسر في كلياته وليست رسمية فالإنسان أكثر منه هو أيديولوجي أكثر من 80 أه يعني شكل لشخصية مستقلة متعددة الجوانب\n",
      "شارك\n",
      "شباب الألتراس كان دوره مهم ملهم في ثورة خمسة وعشرين\n"
     ]
    }
   ],
   "source": [
    "for sentence in EGY[:5]:\n",
    "    print from_buck_to_utf8(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in MSA sentences:  48987\n",
      "Unique words in MSA sentences: 13607\n",
      "في 1739\n",
      "من 1168\n",
      "أن 723\n",
      "على 697\n",
      "إلى 534\n",
      "هذه 384\n",
      "هذا 356\n",
      "التي 354\n",
      "أه 326\n",
      "عن 281\n",
      "ما 272\n",
      "لا 215\n",
      "الذي 214\n",
      "مع 197\n",
      "يعني 190\n",
      "كان 187\n",
      "ال 163\n",
      "أو 163\n",
      "لم 159\n",
      "هل 147\n"
     ]
    }
   ],
   "source": [
    "MSA_words = list()\n",
    "for sentence in MSA:\n",
    "    tokens = from_buck_to_utf8(sentence).split()\n",
    "    MSA_words.extend(tokens)\n",
    "    \n",
    "print 'Total words in MSA sentences: ', len(MSA_words)\n",
    "print 'Unique words in MSA sentences:', len(set(MSA_words))\n",
    "MSA_freq = Counter(MSA_words)\n",
    "for word, freq in MSA_freq.most_common(20):\n",
    "    print word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in EGY sentences:  84949\n",
      "Unique words in EGY sentences: 20836\n",
      "في 2553\n",
      "من 1629\n",
      "يعني 1203\n",
      "على 1091\n",
      "أه 897\n",
      "ما 764\n",
      "أن 693\n",
      "أو 623\n",
      "اللي 590\n",
      "أنا 569\n",
      "كان 537\n",
      "لا 507\n",
      "هو 495\n",
      "هذا 474\n",
      "إن 471\n",
      "ال 452\n",
      "إحنا 430\n",
      "إلى 412\n",
      "مش 393\n",
      "إنه 383\n"
     ]
    }
   ],
   "source": [
    "EGY_words = list()\n",
    "for sentence in EGY:\n",
    "    tokens = from_buck_to_utf8(sentence).split()\n",
    "    EGY_words.extend(tokens)\n",
    "    \n",
    "print 'Total words in EGY sentences: ', len(EGY_words)\n",
    "print 'Unique words in EGY sentences:', len(set(EGY_words))\n",
    "EGY_freq = Counter(EGY_words)\n",
    "for word, freq in EGY_freq.most_common(20):\n",
    "    print word, freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in LAV sentences:  66219\n",
      "Unique words in LAV sentences: 19198\n",
      "في 1362\n",
      "من 1302\n",
      "ما 935\n",
      "على 877\n",
      "يعني 857\n",
      "أه 604\n",
      "اللي 457\n",
      "أنا 455\n",
      "لا 444\n",
      "إنه 441\n",
      "كان 419\n",
      "هذا 418\n",
      "أن 417\n",
      "كل 394\n",
      "أو 388\n",
      "عن 368\n",
      "مع 317\n",
      "هو 304\n",
      "عم 293\n",
      "ال 271\n"
     ]
    }
   ],
   "source": [
    "LAV_words = list()\n",
    "for sentence in LAV:\n",
    "    tokens = from_buck_to_utf8(sentence).split()\n",
    "    LAV_words.extend(tokens)\n",
    "    \n",
    "print 'Total words in LAV sentences: ', len(LAV_words)\n",
    "print 'Unique words in LAV sentences:', len(set(LAV_words))\n",
    "LAV_freq = Counter(LAV_words)\n",
    "for word, freq in LAV_freq.most_common(20):\n",
    "    print word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in NOR sentences:  51593\n",
      "Unique words in NOR sentences: 20271\n",
      "في 1040\n",
      "من 672\n",
      "ما 568\n",
      "أه 516\n",
      "يعني 489\n",
      "اللي 369\n",
      "على 366\n",
      "كان 301\n",
      "كل 285\n",
      "أن 250\n",
      "مش 225\n",
      "أو 214\n",
      "هذا 210\n",
      "مع 201\n",
      "هو 196\n",
      "هي 178\n",
      "أنا 178\n",
      "الله 177\n",
      "لا 173\n",
      "ال 172\n"
     ]
    }
   ],
   "source": [
    "NOR_words = list()\n",
    "for sentence in NOR:\n",
    "    tokens = from_buck_to_utf8(sentence).split()\n",
    "    NOR_words.extend(tokens)\n",
    "    \n",
    "print 'Total words in NOR sentences: ', len(NOR_words)\n",
    "print 'Unique words in NOR sentences:', len(set(NOR_words))\n",
    "NOR_freq = Counter(NOR_words)\n",
    "for word, freq in NOR_freq.most_common(20):\n",
    "    print word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in GLF sentences:  64081\n",
      "Unique words in GLF sentences: 17842\n",
      "في 1852\n",
      "من 1306\n",
      "يعني 1009\n",
      "ما 698\n",
      "أن 666\n",
      "على 666\n",
      "هذا 618\n",
      "أه 494\n",
      "اللي 457\n",
      "لا 447\n",
      "هذه 422\n",
      "أنا 390\n",
      "أو 372\n",
      "إلى 326\n",
      "ال 291\n",
      "عن 288\n",
      "هو 281\n",
      "كان 241\n",
      "مع 233\n",
      "كل 228\n"
     ]
    }
   ],
   "source": [
    "GLF_words = list()\n",
    "for sentence in GLF:\n",
    "    tokens = from_buck_to_utf8(sentence).split()\n",
    "    GLF_words.extend(tokens)\n",
    "    \n",
    "print 'Total words in GLF sentences: ', len(GLF_words)\n",
    "print 'Unique words in GLF sentences:', len(set(GLF_words))\n",
    "GLF_freq = Counter(GLF_words)\n",
    "for word, freq in GLF_freq.most_common(20):\n",
    "    print word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in all sentences:  315829\n",
      "Unique words in all sentences: 55992\n",
      "في 1852\n",
      "من 1306\n",
      "يعني 1009\n",
      "ما 698\n",
      "أن 666\n",
      "على 666\n",
      "هذا 618\n",
      "أه 494\n",
      "اللي 457\n",
      "لا 447\n",
      "هذه 422\n",
      "أنا 390\n",
      "أو 372\n",
      "إلى 326\n",
      "ال 291\n",
      "عن 288\n",
      "هو 281\n",
      "كان 241\n",
      "مع 233\n",
      "كل 228\n"
     ]
    }
   ],
   "source": [
    "sentences\n",
    "words = list()\n",
    "for sentence in sentences:\n",
    "    tokens = from_buck_to_utf8(sentence).split()\n",
    "    words.extend(tokens)\n",
    "    \n",
    "print 'Total words in all sentences: ', len(words)\n",
    "print 'Unique words in all sentences:', len(set(words))\n",
    "freq = Counter(GLF_words)\n",
    "for word, freq in freq.most_common(20):\n",
    "    print word, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Name | Description | age         \n",
      "| :- |-------------: | :-:\n",
      "|Mary| She is a nice girl.  | 20\n",
      "| Jackie Junior | He is a very naughty boy. | 5\n"
     ]
    }
   ],
   "source": [
    "print '| Name | Description | age         '\n",
    "print '| :- |-------------: | :-:'\n",
    "print '|Mary| She is a nice girl.  | 20'\n",
    "print '| Jackie Junior | He is a very naughty boy. | 5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3 align=\"right\">This is a centered header</h3> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<h3 align=\"right\">This is a centered header</h3> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EGY_doc = '. '.join(EGY)\n",
    "LAV_doc = '. '.join(LAV)\n",
    "NOR_doc = '. '.join(NOR)\n",
    "GLF_doc = '. '.join(GLF)\n",
    "MSA_doc = '. '.join(MSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 46027)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vect.fit_transform([EGY_doc,GLF_doc,LAV_doc,NOR_doc,MSA_doc])\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00386453,  0.00124612,  0.        ,  0.        ,  0.00110593],\n",
       "       [ 0.00316051,  0.00086232,  0.        ,  0.00051021,  0.        ],\n",
       "       [ 0.00371368,  0.        ,  0.00041919,  0.        ,  0.        ],\n",
       "       [ 0.00407152,  0.00144415,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.0010686 ,  0.00126343,  0.00045233,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf[:,:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46027"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.matrixlib.defmatrix.matrix'>\n",
      "(5, 46027)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.00386453,  0.00124612,  0.        ,  0.        ,  0.00110593],\n",
       "        [ 0.00316051,  0.00086232,  0.        ,  0.00051021,  0.        ],\n",
       "        [ 0.00371368,  0.        ,  0.00041919,  0.        ,  0.        ],\n",
       "        [ 0.00407152,  0.00144415,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.0010686 ,  0.00126343,  0.00045233,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/\n",
    "\n",
    "dense = X_train_tfidf.todense()\n",
    "print type(dense)\n",
    "print dense.shape\n",
    "\n",
    "dense[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGY:  17640\n",
      "LAV:  16340\n"
     ]
    }
   ],
   "source": [
    "EGY_vec = dense[0].tolist()[0]\n",
    "EGY_phrase_scores = [pair for pair in zip(range(0, len(EGY_vec)), EGY_vec) if pair[1] > 0]\n",
    "print 'EGY: ', len(EGY_phrase_scores)\n",
    "\n",
    "LAV_vec = dense[2].tolist()[0]\n",
    "LAV_phrase_scores = [pair for pair in zip(range(0, len(LAV_vec)), LAV_vec) if pair[1] > 0]\n",
    "print 'LAV: ', len(LAV_phrase_scores)\n",
    "\n",
    "GLF_vec = dense[1].tolist()[0]\n",
    "GLF_phrase_scores = [pair for pair in zip(range(0, len(GLF_vec)), GLF_vec) if pair[1] > 0]\n",
    "\n",
    "NOR_vec = dense[3].tolist()[0]\n",
    "NOR_phrase_scores = [pair for pair in zip(range(0, len(NOR_vec)), NOR_vec) if pair[1] > 0]\n",
    "\n",
    "MSA_vec = dense[4].tolist()[0]\n",
    "MSA_phrase_scores = [pair for pair in zip(range(0, len(MSA_vec)), MSA_vec) if pair[1] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46027"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vect.get_feature_names()\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "َل                   0.613933156569\n",
      "في                   0.459703310656\n",
      "من                   0.316715731415\n",
      "يeني                 0.21131948136\n",
      "eلي                  0.20587400844\n",
      "مَ                   0.159675318833\n",
      "نَ                   0.131042670901\n",
      "لَ                   0.111368704225\n",
      "نه                   0.10943643964\n",
      "َللي                 0.103639645887\n",
      "كَن                  0.0980185125509\n",
      "هنَ                  0.0952079458828\n",
      "كل                   0.0925730396314\n",
      "لي                   0.0890598312963\n",
      "هو                   0.087830208379\n",
      "مe                   0.0655213354507\n",
      "بeد                  0.061656806282\n",
      "eن                   0.0593732208642\n",
      "جدَ                  0.0576166166966\n",
      "مسر                  0.0502388791928\n"
     ]
    }
   ],
   "source": [
    "sorted_EGY_phrase_scores = sorted(EGY_phrase_scores, key=lambda t: t[1] * -1)\n",
    "for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_EGY_phrase_scores][:20]:\n",
    "    #print from_buck_to_utf8(phrase), score\n",
    "    print(u'{0: <20} {1}'.format(from_buck_to_utf8(phrase), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "َل                   0.5719074831\n",
      "في                   0.352552491747\n",
      "من                   0.340916278887\n",
      "مَ                   0.273079633705\n",
      "eلي                  0.226534782267\n",
      "يeني                 0.213165516428\n",
      "نه                   0.190140669706\n",
      "نَ                   0.147804661217\n",
      "لَ                   0.127255604465\n",
      "كل                   0.127008025468\n",
      "َللي                 0.113143601635\n",
      "كَن                  0.104230757742\n",
      "لي                   0.0918518078918\n",
      "eن                   0.0911090709008\n",
      "مe                   0.0787301210501\n",
      "هو                   0.075759173086\n",
      "eم                   0.0725406461248\n",
      "هي                   0.0670939081905\n",
      "كثير                 0.0636278022324\n",
      "بس                   0.0594189592831\n"
     ]
    }
   ],
   "source": [
    "sorted_LAV_phrase_scores = sorted(LAV_phrase_scores, key=lambda t: t[1] * -1)\n",
    "for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_LAV_phrase_scores][:20]:\n",
    "    #print from_buck_to_utf8(phrase), score\n",
    "    print(u'{0: <20} {1}'.format(from_buck_to_utf8(phrase), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
